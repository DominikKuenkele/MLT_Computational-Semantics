{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Translation with different encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most state-of-the-art methods of machine translation use currently an encoder-decoder structure. The encoder tries to find a vector representation for the phrase in the source language and the decoder takes this representation as a basis to generate the phrase in the target language. The goal of the following study is to compare different kinds of encoders for representing the meaning of a source phrase in a vector. For this, I will focus on three different types:\n",
    "- recurrent neural networks (i.e. LSTM) ([3], [4])\n",
    "- transformer ([5], [6])\n",
    "- convolutional neural networks ([1], [2])\n",
    "\n",
    "The structure of the encoders will be based on the work in the referenced papers. For the decoder, I will always use an LSTM, to generate the output sentence. This will allow me, to only compare the differences of the methods in encoding the meaning of a phrase.\n",
    "\n",
    "> Unfortunately, the LSTM didn't produce a very sensible output in the beginning. Consequently, I spent all of my time, investigating the problems and trying to fix and improve the LSTM network. Below, you can find my attempts and experiments and the variables I tried to change. Therefore, I didn't have any time to also build and tweak the transformer or CNN encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0 - Constants/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PADDING_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<SOS>'\n",
    "END_TOKEN = '<EOS>'\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 16,\n",
    "    'embedding_dim': 256,\n",
    "    'lstm_out_dim': 512,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 - Loading Data\n",
    "I will use the Multi30k dataset, which contains source phrases in German and target phrases in English.\n",
    "\n",
    "For this, I downloaded the raw train datasets from https://github.com/multi30k/ in English and German and aligned the translated sentences in one file as following:\n",
    "```\n",
    "[German sentence]\\t[English sentence]\n",
    "[German sentence]\\t[English sentence]\n",
    "...\n",
    "```\n",
    "\n",
    "The following code loads the data from this file, creates train and test sets, tokenizes the sentences and encodes the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MTDataset(Dataset):\n",
    "    def __init__(self, path, max_lines=1000, dataset=None):\n",
    "        data_file = self._read_file(path, max_lines)\n",
    "\n",
    "        if dataset is None:\n",
    "            self.max_length_source = -1\n",
    "            self.max_length_target = -1\n",
    "            vocab_source_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            vocab_target_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            for sample in data_file:\n",
    "                vocab_source_lang.update(sample['vocab_source_lang'])\n",
    "                vocab_target_lang.update(sample['vocab_target_lang'])\n",
    "                self.max_length_source = max(self.max_length_source, len(sample['vocab_source_lang']))\n",
    "                self.max_length_target = max(self.max_length_target, len(sample['vocab_target_lang']))\n",
    "\n",
    "            self.vocab_source_lang = {word: index for index, word in enumerate(list(vocab_source_lang))}\n",
    "            self.vocab_target_lang = {word: index for index, word in enumerate(list(vocab_target_lang))}\n",
    "\n",
    "            # START token, END token\n",
    "            self.max_length_source += 2\n",
    "            self.max_length_target += 2\n",
    "        else:\n",
    "            self.vocab_source_lang = dataset.vocab_source_lang\n",
    "            self.vocab_target_lang = dataset.vocab_target_lang\n",
    "            self.max_length_source = dataset.max_length_source\n",
    "            self.max_length_target = dataset.max_length_target\n",
    "\n",
    "        self.samples = []\n",
    "        for sample in data_file:\n",
    "            source = [self.get_encoded_source_word(word) for word in sample['vocab_source_lang']]\n",
    "            source.insert(0, self.get_encoded_source_word(START_TOKEN))\n",
    "            source.append(self.get_encoded_source_word(END_TOKEN))\n",
    "            source.extend([self.get_encoded_source_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_source - len(sample['vocab_source_lang'])))\n",
    "\n",
    "            target = [self.get_encoded_target_word(word) for word in sample['vocab_target_lang']]\n",
    "            target.insert(0, self.get_encoded_target_word(START_TOKEN))\n",
    "            target.append(self.get_encoded_target_word(END_TOKEN))\n",
    "            target.extend([self.get_encoded_target_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_target - len(sample['vocab_target_lang'])))\n",
    "\n",
    "            self.samples.append({\n",
    "                'source': torch.tensor(source),\n",
    "                'target': torch.tensor(target)\n",
    "            })\n",
    "\n",
    "    def _read_file(self, path, max_lines):\n",
    "        lines = []\n",
    "        with open(path) as f:\n",
    "            for line_index, sample in enumerate(f):\n",
    "                split = sample.rstrip().split('\\t')\n",
    "                if len(split) == 2:\n",
    "                    vocab_source_lang, vocab_target_lang = split\n",
    "                    lines.append({\n",
    "                        'vocab_source_lang': [word.lower() for word in word_tokenize(vocab_source_lang)],\n",
    "                        'vocab_target_lang': [word.lower() for word in word_tokenize(vocab_target_lang)],\n",
    "                    })\n",
    "\n",
    "                    if line_index == max_lines:\n",
    "                        break\n",
    "        return lines\n",
    "\n",
    "    def get_encoded_source_word(self, word):\n",
    "        if word in self.vocab_source_lang:\n",
    "            return self.vocab_source_lang[word]\n",
    "        else:\n",
    "            return self.vocab_source_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_encoded_target_word(self, word):\n",
    "        if word in self.vocab_target_lang:\n",
    "            return self.vocab_target_lang[word]\n",
    "        else:\n",
    "            return self.vocab_target_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_decoded_target_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_target_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def get_decoded_source_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_source_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': tensor([ 978,  919,  571,  781, 1140,  236, 1421,  175, 1533,  910, 1460,  563,\n",
      "        1238,  543,   33, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793, 1269,   71, 1165,  863, 1598,   10, 1044,  177, 1774,  995,  425,\n",
      "          20, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978,  233, 1140, 1912,  986, 1127, 1186, 1891,  543,   33, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  792, 1799, 1265, 1176,   41,   10, 1222,  747, 1870, 1400,  373,\n",
      "         425,   20, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1186, 1679,  170,  504, 1533, 1186, 1729, 1117,  535,  543,   33,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747,  243, 1837,  680,  942,  747, 1297,  296,  425,   20, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1186, 1886, 1533, 1450,  390, 1419, 1288, 1592, 1814,  190,  761,\n",
      "        1617, 1186, 1273,  543,   33, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747, 1710, 1265,  747, 1133,   32, 1320, 1678,  509,  747,  210,\n",
      "         370,  747,  174,  425,   20, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978,  919, 1140, 2040,  544,   63,  761, 1808, 1191,  786,  543,   33,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793, 1269, 1799,   10,  991,   38, 1193,  503, 1772,  425,   20, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1186, 1886, 1533, 2025,  515, 1324, 1066, 1424, 1364,  910,  721,\n",
      "        1886,  924, 1419, 1307,  543,   33, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747, 1710, 1265,  820,   98,  747, 1343,  298,   38, 1233, 1710,\n",
      "        1754, 1251,   32,  425,   20, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1186, 1886,  949, 1688,  741,  470,   11,  543,   33, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747, 1710, 1320,  112,  991,  747, 1367,  593,   20, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1186, 1124,  170, 1469, 1912,  368, 2098, 1364, 1482,  425, 1887,\n",
      "         463, 1215,  543,   33, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747, 1685, 1837,    0,  509,  184, 1093,  298, 1181,  471, 1059,\n",
      "          38,  348,  425,   20, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978, 1324, 2022, 1912, 1814, 1299, 1723, 1141,   11, 1450,  183,  826,\n",
      "         543,   33, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793,  747,  526, 1850,  747, 1002,  899, 1320, 1621,  950,  747,  794,\n",
      "         425,   20, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}, {'source': tensor([ 978,  520, 1591, 1933, 1533,  910, 1456, 1592,  490,  543,   33, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184,\n",
      "        2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184, 2184]), 'target': tensor([ 793, 1585,  110,  509,  355, 1265,   38,  758,  255,   38,   14,  425,\n",
      "          20, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853, 1853,\n",
      "        1853, 1853, 1853])}]\n"
     ]
    }
   ],
   "source": [
    "dataset = MTDataset('data/multi30k_dev.txt')\n",
    "print(dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(source_path, target_path_train, target_path_test, train_split=0.8):\n",
    "    with open(source_path, 'r') as source:\n",
    "        lines = source.readlines()\n",
    "\n",
    "    delimiter = int(len(lines) * train_split)\n",
    "\n",
    "    with open(target_path_train, 'w') as target_train:\n",
    "        for line in lines[:delimiter]:\n",
    "            target_train.write(line)\n",
    "    with open(target_path_test, 'w') as target_test:\n",
    "        for line in lines[delimiter:]:\n",
    "            target_test.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_data('data/multi30k_dev.txt', 'data/dev_train', 'data/dev_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dataloader(path_train, path_test, batch_size):\n",
    "    train_dataset = MTDataset(path_train, max_lines=-1)\n",
    "    test_dataset = MTDataset(path_test, max_lines=-1, dataset=train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataloader('data/dev_train', 'data/dev_test', hyperparameters['batch_size'])\n",
    "train_dataset = train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 - Models\n",
    "### 2.1 - recurrent neural network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DCEPEncoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dim, encoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(source_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, encoder_out_dim, 8, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, source):\n",
    "        embedding = self.embeddings(source)\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        _, states = self.lstm(dropped_out)\n",
    "\n",
    "        return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DCEPDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, decoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPDecoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(target_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, decoder_out_dim, 8, batch_first=True)\n",
    "        self.classifier = nn.Linear(decoder_out_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, target_word, input_states):\n",
    "        embedding = self.embeddings(target_word.unsqueeze(0).transpose(0,1))\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        output, output_states = self.lstm(dropped_out, input_states)\n",
    "        prediction = self.classifier(output).squeeze(1)\n",
    "\n",
    "        return prediction, output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DCEPSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoded_target_SOS, encoded_target_EOS):\n",
    "        super(DCEPSeq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoded_target_SOS = encoded_target_SOS\n",
    "        self.encoded_target_EOS = encoded_target_EOS\n",
    "\n",
    "    def forward(self, source, target=None):\n",
    "        predicted_sentence = []\n",
    "\n",
    "        states = self.encoder(source)\n",
    "\n",
    "        # predictions during training\n",
    "        if target is not None:\n",
    "            target = target.transpose(0,1)\n",
    "            predicted_word = target[0]\n",
    "            for word in target:\n",
    "                base_word = word if random.random() > 0.5 else predicted_word\n",
    "\n",
    "                predicted_word_layer, states = self.decoder(base_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word_layer)\n",
    "\n",
    "        # predictions during testing/production\n",
    "        else:\n",
    "            sentence_length = 0\n",
    "            predicted_word = torch.tensor([self.encoded_target_SOS] * source.shape[0], device=device)\n",
    "            while predicted_word != torch.tensor(self.encoded_target_EOS, device=device):\n",
    "                predicted_word_layer, states = self.decoder(predicted_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word)\n",
    "                sentence_length += 1\n",
    "\n",
    "                if sentence_length > 30:\n",
    "                    break\n",
    "        return torch.stack(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index=train_dataset.get_encoded_target_word(PADDING_TOKEN))\n",
    "\n",
    "dcepEncoder = DCEPEncoder(len(train_dataset.vocab_source_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_source_word(PADDING_TOKEN),\n",
    "                          0.5)\n",
    "\n",
    "dcepDecoder = DCEPDecoder(len(train_dataset.vocab_target_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_target_word(PADDING_TOKEN),\n",
    "                          0.5)\n",
    "dcepSeq2seq = DCEPSeq2Seq(dcepEncoder,\n",
    "                          dcepDecoder,\n",
    "                          train_dataset.get_encoded_target_word(START_TOKEN),\n",
    "                          train_dataset.get_encoded_target_word(END_TOKEN))\n",
    "dcepSeq2seq.to(device)\n",
    "\n",
    "optimizer = optim.Adam(dcepSeq2seq.parameters(), lr=hyperparameters['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# translate example sentence to see how the translation improves during the training \n",
    "def translate_test():\n",
    "    # <SOS> a man in a blue shirt is standing on a ladder and cleans a window . <EOS>\n",
    "    sentence = \"<SOS> ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .\".split(' ')\n",
    "    encoded_sentence = torch.tensor([train_dataset.get_encoded_source_word(word.lower()) for word in sentence], device=device).unsqueeze(0)\n",
    "    translated = dcepSeq2seq(encoded_sentence).squeeze(0)\n",
    "    return [train_dataset.get_decoded_target_word(int(word)) for word in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 EPOCHS - 50 BATCHES PER EPOCH\n",
      "epoch 0, batch 49: 5.4177\n",
      "['a', 'man', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 1, batch 49: 4.9296\n",
      "['a', 'man', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 2, batch 49: 4.8235\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 3, batch 49: 4.7721\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 4, batch 49: 4.7375\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 5, batch 49: 4.7104\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 6, batch 49: 4.6989\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 7, batch 49: 4.6661\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 8, batch 49: 4.6167\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 9, batch 49: 4.5744\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 10, batch 49: 4.5412\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 11, batch 49: 4.5209\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 12, batch 49: 4.4698\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 13, batch 49: 4.4068\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 14, batch 49: 4.3647\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 15, batch 49: 4.3152\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 16, batch 49: 4.2684\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 17, batch 49: 4.2268\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 18, batch 49: 4.1919\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 19, batch 49: 4.1502\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 20, batch 49: 4.1203\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 21, batch 49: 4.0903\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 22, batch 49: 4.0847\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', '<EOS>']\n",
      "epoch 23, batch 49: 4.1088\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 24, batch 49: 4.1327\n",
      "['two', 'men', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 25, batch 49: 4.0839\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 26, batch 49: 4.1219\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 27, batch 49: 4.0863\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 28, batch 49: 4.0274\n",
      "['two', 'people', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 29, batch 49: 4.0485\n",
      "['two', 'people', ',', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 30, batch 49: 4.0363\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 31, batch 49: 4.0067\n",
      "['two', 'people', ',', 'a', 'a', 'a', 'a', ',', 'a', '.', '<EOS>']\n",
      "epoch 32, batch 49: 3.9839\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 33, batch 49: 3.9714\n",
      "['two', 'men', ',', 'a', 'a', 'a', 'a', ',', 'a', '.', '<EOS>']\n",
      "epoch 34, batch 49: 3.9546\n",
      "['two', 'people', ',', 'a', 'a', 'a', 'a', ',', 'a', '.', '.', '.', '.', '<EOS>']\n",
      "epoch 35, batch 49: 3.9405\n",
      "['a', 'man', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 36, batch 49: 3.9311\n",
      "['two', 'women', ',', 'a', 'a', 'a', 'a', ',', 'a', '.', 'the', '.', '.', '<EOS>']\n",
      "epoch 37, batch 49: 3.9318\n",
      "['two', 'man', ',', 'a', 'a', 'a', 'a', ',', 'a', 'and', 'a', 'a', 'washing', '.', '.', '<EOS>']\n",
      "epoch 38, batch 49: 3.9422\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 39, batch 49: 3.9337\n",
      "['two', 'people', ',', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 40, batch 49: 3.9115\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 41, batch 49: 3.9047\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 42, batch 49: 3.8936\n",
      "['a', 'young', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 43, batch 49: 3.8823\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 44, batch 49: 3.8816\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 45, batch 49: 3.8696\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 46, batch 49: 3.8581\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 47, batch 49: 3.8566\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 48, batch 49: 3.8551\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 49, batch 49: 3.8462\n",
      "['a', 'woman', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 50, batch 49: 3.8348\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 51, batch 49: 3.8327\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 52, batch 49: 3.8122\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 53, batch 49: 3.8072\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 54, batch 49: 3.7925\n",
      "['a', 'group', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 55, batch 49: 3.7894\n",
      "['a', 'group', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 56, batch 49: 3.7824\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 57, batch 49: 3.7666\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 58, batch 49: 3.7706\n",
      "['a', 'man', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 59, batch 49: 3.7648\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 60, batch 49: 3.7845\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 61, batch 49: 3.7587\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 62, batch 49: 3.7568\n",
      "['a', 'man', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 63, batch 49: 3.7538\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 64, batch 49: 3.7767\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 65, batch 49: 3.7477\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 66, batch 49: 3.7808\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 67, batch 49: 3.7978\n",
      "['a', 'woman', 'of', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 68, batch 49: 3.7961\n",
      "['two', 'people', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 69, batch 49: 3.7979\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 70, batch 49: 3.8022\n",
      "['two', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 71, batch 49: 3.7929\n",
      "['a', 'man', 'and', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 72, batch 49: 3.7864\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 73, batch 49: 3.7893\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 74, batch 49: 3.7802\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 75, batch 49: 3.8458\n",
      "['a', 'man', 'is', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 76, batch 49: 3.9617\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 77, batch 49: 3.8581\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 78, batch 49: 3.8009\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 79, batch 49: 3.7711\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 80, batch 49: 3.7632\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 81, batch 49: 3.7397\n",
      "['a', 'group', 'of', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 82, batch 49: 3.7211\n",
      "['two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 83, batch 49: 3.7289\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 84, batch 49: 3.7179\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 85, batch 49: 3.7121\n",
      "['two', 'people', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 86, batch 49: 3.7029\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 87, batch 49: 3.7022\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '<EOS>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88, batch 49: 3.7029\n",
      "['a', 'woman', 'in', 'a', 'a', 'shirt', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 89, batch 49: 3.7141\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 90, batch 49: 3.7115\n",
      "['a', 'woman', 'woman', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 91, batch 49: 3.6966\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 92, batch 49: 3.6908\n",
      "['a', 'woman', 'woman', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 93, batch 49: 3.9604\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 94, batch 49: 3.9332\n",
      "['a', 'woman', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 95, batch 49: 4.0727\n",
      "['a', 'woman', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 96, batch 49: 4.0589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 97, batch 49: 4.0059\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', '<EOS>']\n",
      "epoch 98, batch 49: 3.8607\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 99, batch 49: 3.8506\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 100, batch 49: 3.7828\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 101, batch 49: 3.7627\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 102, batch 49: 3.7539\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 103, batch 49: 3.7543\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 104, batch 49: 3.8675\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'is', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 105, batch 49: 4.0366\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 106, batch 49: 3.9999\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 107, batch 49: 3.9949\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'is', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 108, batch 49: 3.9868\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 109, batch 49: 3.9836\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'is', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 110, batch 49: 3.9798\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 111, batch 49: 3.9774\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', '<EOS>']\n",
      "epoch 112, batch 49: 3.9744\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 113, batch 49: 3.9728\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 114, batch 49: 3.9737\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 115, batch 49: 3.9679\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 116, batch 49: 3.9579\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 117, batch 49: 3.9305\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 118, batch 49: 3.9206\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 119, batch 49: 3.9724\n",
      "['a', 'woman', 'of', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 120, batch 49: 3.9664\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 121, batch 49: 3.9616\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 122, batch 49: 3.9621\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 123, batch 49: 3.9604\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 124, batch 49: 3.9619\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 125, batch 49: 3.9578\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 126, batch 49: 3.9582\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 127, batch 49: 3.9564\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 128, batch 49: 3.9534\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 129, batch 49: 3.9561\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 130, batch 49: 3.9529\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 131, batch 49: 3.9539\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 132, batch 49: 3.9526\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'is', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 133, batch 49: 3.9507\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 134, batch 49: 3.9496\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 135, batch 49: 3.9486\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 136, batch 49: 3.9495\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 137, batch 49: 3.9503\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 138, batch 49: 3.9492\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 139, batch 49: 3.9497\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 140, batch 49: 3.9473\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 141, batch 49: 3.9466\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>']\n",
      "epoch 142, batch 49: 3.9478\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 143, batch 49: 3.9441\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 144, batch 49: 3.9455\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 145, batch 49: 3.9449\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 146, batch 49: 3.9447\n",
      "['a', 'woman', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 147, batch 49: 3.9435\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n",
      "epoch 148, batch 49: 3.9444\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 149, batch 49: 3.9417\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(f'{hyperparameters[\"epochs\"]} EPOCHS - {math.floor(len(train_dataset) / train_dataloader.batch_size)} BATCHES PER EPOCH')\n",
    "\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        source = batch['source'].to(device)\n",
    "        target = batch['target'].type(torch.LongTensor).to(device)\n",
    "\n",
    "        output = dcepSeq2seq(source, target)\n",
    "#        print(output.transpose(0,1).size())\n",
    "#        print(target.size())\n",
    "#        print(torch.max(output.transpose(0,1), 2).indices.size())\n",
    "#        print()\n",
    "#        print([train_dataset.get_decoded_source_word(int(word)) for word in source[0]])\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in target[:, 1:][0]])\n",
    "#        max_output = torch.max(output.transpose(0,1)[:, :-1], 2).indices\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in max_output[0]])\n",
    "        loss = loss_function(output.transpose(0,1)[:, :-1].reshape(-1, output.shape[2]), target[:, 1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        sys.stdout.write(f'\\repoch {epoch}, batch {i}: {np.round(total_loss / (i + 1), 4)}')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()\n",
    "    print(translate_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "dcepSeq2seq.eval()\n",
    "print(translate_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 - transformer\n",
    "*future work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 - convolutional neural network\n",
    "*future work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 - Evaluation\n",
    "*future work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 - Discussion\n",
    "As can be seen, the translation doesn't work well. The first few words are translated correctly, but thenit stops predicting the correct words. Since, there is already a big descrepancy, I didn't implement a proper test with metrics like the *bleu score* or similar. This certainly needs to be done in future work when it becomes visible that the model works correctly. There are multiple things, I tried to improve and fix the model. \n",
    "\n",
    "The first thing, I tried, was changing the data set. In the beginning I used the DCEP corpus (Digital Corpus of the European Parliament), but after no success, I changed it to the Multi30k corpus. This corpus was used in multiple papers and models for translation. In the end, this didn't improve the model. Taking this step further, I also created my own 'corpus', which consisted of just two translated sentences. For this test, I used a big number of epochs and no dropout. In theory, the model should now learn the corpus by heart. This experiemnt failed, since the model could only translate one sentence, while outputting a wrong translation for the other sentence. Therefore, the problem needed to be somewhere else.\n",
    "\n",
    "Next, I tried to adapt the hyperparameters, namely the batch size, learning rate, dimensions and dropout. The **batch size** didn't have a big effect on the results, when kept in between 16 and 256 (it was just slowing the training/overflowing the memory). Changing the **dimensions** was more difficult to test, since higher dimensions immediatly prolonged the training time for hours. I used some trial and error (with some trainings running for 20 hours), but while higher dimensions performed somewhat better, they still didn't produce any complete sentences and were in my perspective not the biggest problem. For a better training time, i kept them lower, but for tweaking the models performance, they may be adapted in future work. The **learning rate** had a much higher impact. I started with a learning rate of around *0,02*. Lowering this to something between *0,002* to *0.0002* had a major impact on the cross entropy loss. While the loss was converging to around *7* in the beginning, it now converged to around *3,6*. But still, the model was unable to produce complete sentences and wasn't able to get the loss lower after around the 100th epoch.\n",
    "\n",
    "Debugging the whole E2E model seemed very hard to me, since I couldn't pin down the location of the problem. It could be part of the encoder, the decoder, the combination or even the data loading. From observing the translated test sentences, it nonetheless seems to me, that the problem lies in the encoder. The decoder is able to predict word by word and consistently also predicts and <EOS> token. The encoder somehow seems to fail encoding the middle and end part of the sequence, while encoding the beginning of the sequence quite well. Future work may test this, by using a training corpus with only very short sentences, to see how much the LSTM encoder can remember. Furthermore, it may be possible to connect a different encoder (e.g. a transformer) to rule out problems of the decoder. Another try could be using closer related languages, e.g. German/Swedish or Italian/Spanish. This may be a simpler problem to solve for the model. With more computational resources, it would also be very interesting to test different hyperparamters more systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "[1] Gehring et al. 2017. Convolutional Sequence to Sequence Learning\n",
    "[2] Gehring et al. 2017. A Convolutional Encoder Model for Neural Machine Translation\n",
    "[3] Cho et al. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "[4] Zhou et al. 2016. Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
    "[5] Zhou et al. 2020. Incorporating BERT into Neural Machine Translation\n",
    "[6] Vaswani et al. 2017. Attention is All you Need"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
