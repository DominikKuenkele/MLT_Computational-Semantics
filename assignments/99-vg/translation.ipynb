{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Translation with different encoders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> THIS IS A DRAFT - Unfortunately, I couldn't implement and evaluate all different methods so far. So this file contains only the first steps of creating the RNN encoder/decoder."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most state-of-the-art methods of machine translation use currently an encoder-decoder structure. The encoder tries to find a vector representation for the phrase in the source language and the decoder takes this representation as a basis to generate the phrase in the target language. The goal of the following study is to compare different kinds of encoders for representing the meaning of a source phrase in a vector. For this, I will focus on three different types:\n",
    "- recurrent neural networks (i.e. LSTM) ([3], [4])\n",
    "- transformer ([5], [6])\n",
    "- convolutional neural networks ([1], [2])\n",
    "\n",
    "The structure of the encoders will be based on the work in the referenced papers. For the decoder, I will always use an LSTM, to generate the output sentence. This will allow me, to only compare the differences of the methods in encoding the meaning of a phrase."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 - Constants/Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "PADDING_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<SOS>'\n",
    "END_TOKEN = '<EOS>'\n",
    "\n",
    "device = torch.device('cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 128,\n",
    "    'embedding_dim': 256,\n",
    "    'lstm_out_dim': 512,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 0.002\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Loading Data\n",
    "I will use the Multi30k dataset, which contains source phrases in German and target phrases in English."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "class MTDataset(Dataset):\n",
    "    def __init__(self, path, max_lines=1000, dataset=None):\n",
    "        data_file = self._read_file(path, max_lines)\n",
    "\n",
    "        if dataset is None:\n",
    "            self.max_length_source = -1\n",
    "            self.max_length_target = -1\n",
    "            vocab_source_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            vocab_target_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            for sample in data_file:\n",
    "                vocab_source_lang.update(sample['vocab_source_lang'])\n",
    "                vocab_target_lang.update(sample['vocab_target_lang'])\n",
    "                self.max_length_source = max(self.max_length_source, len(sample['vocab_source_lang']))\n",
    "                self.max_length_target = max(self.max_length_target, len(sample['vocab_target_lang']))\n",
    "\n",
    "            self.vocab_source_lang = {word: index for index, word in enumerate(list(vocab_source_lang))}\n",
    "            self.vocab_target_lang = {word: index for index, word in enumerate(list(vocab_target_lang))}\n",
    "\n",
    "            # START token, END token\n",
    "            self.max_length_source += 2\n",
    "            self.max_length_target += 2\n",
    "        else:\n",
    "            self.vocab_source_lang = dataset.vocab_source_lang\n",
    "            self.vocab_target_lang = dataset.vocab_target_lang\n",
    "            self.max_length_source = dataset.max_length_source\n",
    "            self.max_length_target = dataset.max_length_target\n",
    "\n",
    "        self.samples = []\n",
    "        for sample in data_file:\n",
    "            source = [self.get_encoded_source_word(word) for word in sample['vocab_source_lang']]\n",
    "            source.insert(0, self.get_encoded_source_word(START_TOKEN))\n",
    "            source.append(self.get_encoded_source_word(END_TOKEN))\n",
    "            source.extend([self.get_encoded_source_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_source - len(sample['vocab_source_lang'])))\n",
    "\n",
    "            target = [self.get_encoded_target_word(word) for word in sample['vocab_target_lang']]\n",
    "            target.insert(0, self.get_encoded_target_word(START_TOKEN))\n",
    "            target.append(self.get_encoded_target_word(END_TOKEN))\n",
    "            target.extend([self.get_encoded_target_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_target - len(sample['vocab_target_lang'])))\n",
    "\n",
    "            self.samples.append({\n",
    "                'source': torch.tensor(source),\n",
    "                'target': torch.tensor(target)\n",
    "            })\n",
    "\n",
    "    def _read_file(self, path, max_lines):\n",
    "        lines = []\n",
    "        with open(path) as f:\n",
    "            for line_index, sample in enumerate(f):\n",
    "                split = sample.rstrip().split('\\t')\n",
    "                if len(split) == 2:\n",
    "                    vocab_source_lang, vocab_target_lang = split\n",
    "                    lines.append({\n",
    "                        'vocab_source_lang': [word.lower() for word in word_tokenize(vocab_source_lang)],\n",
    "                        'vocab_target_lang': [word.lower() for word in word_tokenize(vocab_target_lang)],\n",
    "                    })\n",
    "\n",
    "                    if line_index == max_lines:\n",
    "                        break\n",
    "        return lines\n",
    "\n",
    "    def get_encoded_source_word(self, word):\n",
    "        if word in self.vocab_source_lang:\n",
    "            return self.vocab_source_lang[word]\n",
    "        else:\n",
    "            return self.vocab_source_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_encoded_target_word(self, word):\n",
    "        if word in self.vocab_target_lang:\n",
    "            return self.vocab_target_lang[word]\n",
    "        else:\n",
    "            return self.vocab_target_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_decoded_target_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_target_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def get_decoded_source_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_source_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': tensor([1499, 2148,  281,  849, 1696,  616,  227,  675,  798, 1901, 1693, 1978,\n",
      "        2146, 1854,  728,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  910, 1306, 1525,  572,    9, 1290,  299, 1246,  528, 1680, 1555,\n",
      "         624,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  854, 1696, 1682, 1711, 1203,  869,  716, 1854,  728,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245, 1045, 1497,  683,  100, 1099, 1290,  514,  165,  936, 1039,  907,\n",
      "        1555,  624,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  869,   89, 1536, 1790,  798,  869, 1674,  246,  503, 1854,  728,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165,  710,  759,   41, 1309,  165, 1592, 1334, 1555,  624,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  869, 1342,  798,  749,  429, 1761, 1348,  508,  628,  491, 1984,\n",
      "        1059,  869, 1033, 1854,  728,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165, 1325,  683,  165, 1749, 1582,  371,  548, 1635,  165, 1756,\n",
      "         574,  165,  587, 1555,  624,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499, 2148, 1696, 1879,  971, 1663, 1984, 1555, 1404,  624, 1854,  728,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  910, 1497, 1290, 1561, 1197,  870,  510, 1209, 1555,  624,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  869, 1342,  798,   72,  505, 1047,  743, 1821, 1917, 1901,  651,\n",
      "        1342, 1925, 1761, 1227, 1854,  728,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165, 1325,  683,  295,  223,  165,  930,  330, 1197,  586, 1325,\n",
      "        1129,  585, 1582, 1555,  624,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  869, 1342, 1700,  735,  715, 1687,  901, 1854,  728,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165, 1325,  371,  613, 1561,  165,  119, 1042,  624,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  869,  158, 1536,  642, 1682, 1425,  445, 1917, 1621, 1547,  137,\n",
      "         779,   67, 1854,  728,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165,  348,  759, 1643, 1635, 1517,  464,  330,   32, 1420,  602,\n",
      "        1197,  563, 1555,  624,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499, 1047,  425, 1682,  628,  699,  434, 1519,  901,  749,  763, 1086,\n",
      "        1854,  728,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  165,  855,  280,  165,  291, 1161,  371,  394, 1249,  165,  984,\n",
      "        1555,  624,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}, {'source': tensor([1499,  915, 2087, 1545,  798, 1901, 1036,  508,  689, 1854,  728,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,\n",
      "         245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245,  245]), 'target': tensor([1245,  649,  962, 1635, 1695,  683, 1197,  122,  810, 1197,  600, 1555,\n",
      "         624,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,  209,\n",
      "         209,  209,  209])}]\n"
     ]
    }
   ],
   "source": [
    "dataset = MTDataset('data/multi30k.txt')\n",
    "print(dataset[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def split_data(source_path, target_path_train, target_path_test, train_split=0.8):\n",
    "    with open(source_path, 'r') as source:\n",
    "        lines = source.readlines()\n",
    "\n",
    "    delimiter = int(len(lines) * train_split)\n",
    "\n",
    "    with open(target_path_train, 'w') as target_train:\n",
    "        for line in lines[:delimiter]:\n",
    "            target_train.write(line)\n",
    "    with open(target_path_test, 'w') as target_test:\n",
    "        for line in lines[delimiter:]:\n",
    "            target_test.write(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "split_data('data/multi30k.txt', 'data/dev_train', 'data/dev_test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "def dataloader(path_train, path_test, batch_size):\n",
    "    train_dataset = MTDataset(path_train, max_lines=-1)\n",
    "    test_dataset = MTDataset(path_test, max_lines=-1, dataset=train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataloader('data/dev_train', 'data/dev_test', hyperparameters['batch_size'])\n",
    "train_dataset = train_dataloader.dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 - Models\n",
    "### 2.1 - recurrent neural network (LSTM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "class DCEPEncoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dim, encoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(source_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, encoder_out_dim, 8, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, source):\n",
    "        embedding = self.embeddings(source)\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        _, states = self.lstm(dropped_out)\n",
    "\n",
    "        return states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "class DCEPDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, decoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPDecoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(target_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, decoder_out_dim, 8, batch_first=True)\n",
    "        self.classifier = nn.Linear(decoder_out_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, target_word, input_states):\n",
    "        embedding = self.embeddings(target_word.unsqueeze(0).transpose(0,1))\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        output, output_states = self.lstm(dropped_out, input_states)\n",
    "        prediction = self.classifier(output).squeeze(1)\n",
    "\n",
    "        return prediction, output_states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "class DCEPSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoded_target_SOS, encoded_target_EOS):\n",
    "        super(DCEPSeq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoded_target_SOS = encoded_target_SOS\n",
    "        self.encoded_target_EOS = encoded_target_EOS\n",
    "\n",
    "    def forward(self, source, target=None):\n",
    "        predicted_sentence = []\n",
    "\n",
    "        states = self.encoder(source)\n",
    "\n",
    "        if target is not None:\n",
    "            target = target.transpose(0,1)\n",
    "            predicted_word = target[0]\n",
    "            for word in target:\n",
    "                base_word = word if random.random() > 0.5 else predicted_word\n",
    "\n",
    "                predicted_word_layer, states = self.decoder(base_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word_layer)\n",
    "        else:\n",
    "            sentence_length = 0\n",
    "            predicted_word = torch.tensor([self.encoded_target_SOS] * source.shape[0], device=device)\n",
    "            while predicted_word != torch.tensor(self.encoded_target_EOS, device=device):\n",
    "                predicted_word_layer, states = self.decoder(predicted_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word)\n",
    "                sentence_length += 1\n",
    "\n",
    "                if sentence_length > 30:\n",
    "                    break\n",
    "        return torch.stack(predicted_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index=train_dataset.get_encoded_source_word(PADDING_TOKEN))\n",
    "\n",
    "dcepEncoder = DCEPEncoder(len(train_dataset.vocab_source_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_source_word(PADDING_TOKEN),\n",
    "                          0)\n",
    "\n",
    "dcepDecoder = DCEPDecoder(len(train_dataset.vocab_target_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_target_word(PADDING_TOKEN),\n",
    "                          0)\n",
    "dcepSeq2seq = DCEPSeq2Seq(dcepEncoder,\n",
    "                          dcepDecoder,\n",
    "                          train_dataset.get_encoded_target_word(START_TOKEN),\n",
    "                          train_dataset.get_encoded_target_word(END_TOKEN))\n",
    "dcepSeq2seq.to(device)\n",
    "\n",
    "optimizer = optim.Adam(dcepSeq2seq.parameters(), lr=hyperparameters['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "def translate_test():\n",
    "    sentence = \"<SOS> Eine Frau mit einer schwarzen Jacke sitzt und spielt Klavier .\".split(' ')\n",
    "    encoded_sentence = torch.tensor([train_dataset.get_encoded_source_word(word.lower()) for word in sentence], device=device).unsqueeze(0)\n",
    "    translated = dcepSeq2seq(encoded_sentence).squeeze(0)\n",
    "    return [train_dataset.get_decoded_target_word(int(word)) for word in translated]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 EPOCHS - 181 BATCHES PER EPOCH\n",
      "epoch 0, batch 181: 1.9846\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 1, batch 181: 1.8242\n",
      "['a', 'man', 'in', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 2, batch 181: 1.8156\n",
      "['a', 'man', 'in', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 3, batch 181: 1.8102\n",
      "['a', 'man', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 4, batch 181: 1.8045\n",
      "['a', 'man', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 5, batch 181: 1.8027\n",
      "['a', 'man', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 6, batch 181: 1.7981\n",
      "['a', 'man', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 7, batch 181: 1.7953\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 8, batch 181: 1.7923\n",
      "['a', 'man', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 9, batch 181: 1.7912\n",
      "['a', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 10, batch 181: 1.7887\n",
      "['a', 'man', 'in', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 11, batch 181: 1.7871\n",
      "['a', 'man', 'in', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 12, batch 181: 1.7878\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 13, batch 181: 1.7849\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 14, batch 181: 1.7829\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 15, batch 181: 1.7821\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 16, batch 181: 1.7814\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 17, batch 181: 1.7838\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 18, batch 181: 1.7797\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 19, batch 181: 1.7791\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 20, batch 181: 1.7779\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 21, batch 181: 1.7772\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 22, batch 181: 1.7764\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 23, batch 181: 1.7754\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 24, batch 181: 1.7741\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 25, batch 181: 1.7747\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 26, batch 181: 1.7727\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 27, batch 181: 1.7723\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 28, batch 181: 1.7726\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 29, batch 181: 1.7711\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 30, batch 181: 1.7717\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 31, batch 181: 1.7709\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 32, batch 181: 1.7703\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 33, batch 181: 1.7697\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 34, batch 181: 1.7692\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 35, batch 181: 1.7692\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 36, batch 181: 1.7687\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 37, batch 181: 1.7678\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 38, batch 181: 1.7669\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 39, batch 181: 1.7679\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 40, batch 181: 1.7684\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 41, batch 181: 1.7662\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 42, batch 181: 1.7672\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 43, batch 181: 1.7656\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 44, batch 181: 1.7662\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 45, batch 181: 1.7656\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 46, batch 181: 1.7653\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 47, batch 181: 1.7816\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 48, batch 181: 1.7686\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 49, batch 181: 1.7675\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 50, batch 181: 1.7645\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 51, batch 181: 1.7649\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 52, batch 181: 1.7643\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 53, batch 181: 1.7637\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 54, batch 181: 1.7642\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 55, batch 181: 1.7636\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 56, batch 181: 1.7631\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 57, batch 181: 1.7628\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 58, batch 181: 1.7633\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 59, batch 181: 1.7628\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 60, batch 181: 1.7629\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 61, batch 181: 1.7631\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 62, batch 181: 1.7635\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 63, batch 181: 1.7617\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 64, batch 181: 1.7624\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 65, batch 181: 1.7619\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 66, batch 181: 1.7621\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 67, batch 181: 1.7623\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 68, batch 181: 1.7613\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 69, batch 181: 1.7617\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 70, batch 181: 1.7617\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 71, batch 181: 1.7621\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 72, batch 181: 1.7614\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 73, batch 181: 1.7612\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 74, batch 181: 1.7607\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 75, batch 181: 1.7605\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 76, batch 181: 1.7602\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 77, batch 181: 1.7613\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 78, batch 181: 1.7608\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 79, batch 181: 1.7605\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 80, batch 181: 1.7611\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 81, batch 181: 1.7599\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 82, batch 181: 1.7598\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 83, batch 181: 1.7607\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 84, batch 181: 1.7599\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 85, batch 181: 1.7607\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 86, batch 181: 1.7601\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 87, batch 181: 1.7598\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 88, batch 181: 1.7601\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 89, batch 181: 1.7595\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 90, batch 181: 1.7596\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 91, batch 181: 1.7596\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 92, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 93, batch 181: 1.7598\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 94, batch 181: 1.7587\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 95, batch 181: 1.7596\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 96, batch 181: 1.7591\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 97, batch 181: 1.7592\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 98, batch 181: 1.7593\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 99, batch 181: 1.7599\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 100, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 101, batch 181: 1.7597\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 102, batch 181: 1.7587\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 103, batch 181: 1.7588\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 104, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 105, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 106, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 107, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 108, batch 181: 1.7587\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 109, batch 181: 1.7581\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 110, batch 181: 1.7589\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 111, batch 181: 1.7579\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 112, batch 181: 1.7597\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 113, batch 181: 1.7586\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 114, batch 181: 1.7582\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 115, batch 181: 1.7578\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 116, batch 181: 1.7583\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 117, batch 181: 1.7586\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 118, batch 181: 1.7575\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 119, batch 181: 1.7582\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 120, batch 181: 1.7577\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 121, batch 181: 1.7573\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 122, batch 181: 1.7576\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 123, batch 181: 1.7582\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 124, batch 181: 1.7575\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 125, batch 181: 1.7571\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 126, batch 181: 1.7575\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 127, batch 181: 1.7588\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 128, batch 181: 1.7576\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 129, batch 181: 1.7571\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 130, batch 181: 1.7579\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 131, batch 181: 1.7573\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 132, batch 181: 1.7577\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 133, batch 181: 1.7576\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 134, batch 181: 1.7576\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 135, batch 181: 1.7577\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 136, batch 181: 1.7574\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 137, batch 181: 1.7566\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 138, batch 181: 1.7574\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 139, batch 181: 1.7571\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 140, batch 181: 1.7567\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 141, batch 181: 1.7576\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 142, batch 181: 1.7567\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 143, batch 181: 1.7575\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 144, batch 181: 1.7565\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 145, batch 181: 1.7573\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 146, batch 181: 1.7587\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 147, batch 181: 1.7568\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 148, batch 181: 1.7571\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "epoch 149, batch 181: 1.7567\n",
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(f'{hyperparameters[\"epochs\"]} EPOCHS - {math.floor(len(train_dataset) / train_dataloader.batch_size)} BATCHES PER EPOCH')\n",
    "\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        source = batch['source'].to(device)\n",
    "        target = batch['target'].type(torch.LongTensor).to(device)\n",
    "\n",
    "        output = dcepSeq2seq(source, target)\n",
    "#        print(output.transpose(0,1).size())\n",
    "#        print(target.size())\n",
    "#        print(torch.max(output.transpose(0,1), 2).indices.size())\n",
    "#        print()\n",
    "#        print([train_dataset.get_decoded_source_word(int(word)) for word in source[0]])\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in target[:, 1:][0]])\n",
    "#        max_output = torch.max(output.transpose(0,1)[:, :-1], 2).indices\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in max_output[0]])\n",
    "        loss = loss_function(output.transpose(0,1)[:, :-1].reshape(-1, output.shape[2]), target[:, 1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        sys.stdout.write(f'\\repoch {epoch}, batch {i}: {np.round(total_loss / (i + 1), 4)}')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()\n",
    "    print(translate_test())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'in', 'a', 'a', 'a', 'a', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "dcepSeq2seq.eval()\n",
    "print(translate_test())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 - transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 - convolutional neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 - Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 - Discussion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "[1] Gehring et al. 2017. Convolutional Sequence to Sequence Learning\n",
    "[2] Gehring et al. 2017. A Convolutional Encoder Model for Neural Machine Translation\n",
    "[3] Cho et al. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "[4] Zhou et al. 2016. Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
    "[5] Zhou et al. 2020. Incorporating BERT into Neural Machine Translation\n",
    "[6] Vaswani et al. 2017. Attention is All you Need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}