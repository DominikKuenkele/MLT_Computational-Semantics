{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Translation with different encoders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> THIS IS A DRAFT - Unfortunately, I couldn't implement and evaluate all different methods so far. So this file contains only the first steps of creating the RNN encoder/decoder."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most state-of-the-art methods of machine translation use currently an encoder-decoder structure. The encoder tries to find a vector representation for the phrase in the source language and the decoder takes this representation as a basis to generate the phrase in the target language. The goal of the following study is to compare different kinds of encoders for representing the meaning of a source phrase in a vector. For this, I will focus on three different types:\n",
    "- recurrent neural networks (i.e. LSTM) ([3], [4])\n",
    "- transformer ([5], [6])\n",
    "- convolutional neural networks ([1], [2])\n",
    "\n",
    "The structure of the encoders will be based on the work in the referenced papers. For the decoder, I will always use an LSTM, to generate the output sentence. This will allow me, to only compare the differences of the methods in encoding the meaning of a phrase."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 - Constants/Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PADDING_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<SOS>'\n",
    "END_TOKEN = '<EOS>'\n",
    "\n",
    "device = torch.device('cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 128,\n",
    "    'embedding_dim': 256,\n",
    "    'lstm_out_dim': 512,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 0.002\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Loading Data\n",
    "I will use the Multi30k dataset, which contains source phrases in German and target phrases in English."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class MTDataset(Dataset):\n",
    "    def __init__(self, path, max_lines=1000, dataset=None):\n",
    "        data_file = self._read_file(path, max_lines)\n",
    "\n",
    "        if dataset is None:\n",
    "            self.max_length_source = -1\n",
    "            self.max_length_target = -1\n",
    "            vocab_source_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            vocab_target_lang = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN}\n",
    "            for sample in data_file:\n",
    "                vocab_source_lang.update(sample['vocab_source_lang'])\n",
    "                vocab_target_lang.update(sample['vocab_target_lang'])\n",
    "                self.max_length_source = max(self.max_length_source, len(sample['vocab_source_lang']))\n",
    "                self.max_length_target = max(self.max_length_target, len(sample['vocab_target_lang']))\n",
    "\n",
    "            self.vocab_source_lang = {word: index for index, word in enumerate(list(vocab_source_lang))}\n",
    "            self.vocab_target_lang = {word: index for index, word in enumerate(list(vocab_target_lang))}\n",
    "\n",
    "            # START token, END token\n",
    "            self.max_length_source += 2\n",
    "            self.max_length_target += 2\n",
    "        else:\n",
    "            self.vocab_source_lang = dataset.vocab_source_lang\n",
    "            self.vocab_target_lang = dataset.vocab_target_lang\n",
    "            self.max_length_source = dataset.max_length_source\n",
    "            self.max_length_target = dataset.max_length_target\n",
    "\n",
    "        self.samples = []\n",
    "        for sample in data_file:\n",
    "            source = [self.get_encoded_source_word(word) for word in sample['vocab_source_lang']]\n",
    "            source.insert(0, self.get_encoded_source_word(START_TOKEN))\n",
    "            source.append(self.get_encoded_source_word(END_TOKEN))\n",
    "            source.extend([self.get_encoded_source_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_source - len(sample['vocab_source_lang'])))\n",
    "\n",
    "            target = [self.get_encoded_target_word(word) for word in sample['vocab_target_lang']]\n",
    "            target.insert(0, self.get_encoded_target_word(START_TOKEN))\n",
    "            target.append(self.get_encoded_target_word(END_TOKEN))\n",
    "            target.extend([self.get_encoded_target_word(PADDING_TOKEN)] * (\n",
    "                    self.max_length_target - len(sample['vocab_target_lang'])))\n",
    "\n",
    "            self.samples.append({\n",
    "                'source': torch.tensor(source),\n",
    "                'target': torch.tensor(target)\n",
    "            })\n",
    "\n",
    "    def _read_file(self, path, max_lines):\n",
    "        lines = []\n",
    "        with open(path) as f:\n",
    "            for line_index, sample in enumerate(f):\n",
    "                split = sample.rstrip().split('\\t')\n",
    "                if len(split) == 2:\n",
    "                    vocab_source_lang, vocab_target_lang = split\n",
    "                    lines.append({\n",
    "                        'vocab_source_lang': [word.lower() for word in word_tokenize(vocab_source_lang)],\n",
    "                        'vocab_target_lang': [word.lower() for word in word_tokenize(vocab_target_lang)],\n",
    "                    })\n",
    "\n",
    "                    if line_index == max_lines:\n",
    "                        break\n",
    "        return lines\n",
    "\n",
    "    def get_encoded_source_word(self, word):\n",
    "        if word in self.vocab_source_lang:\n",
    "            return self.vocab_source_lang[word]\n",
    "        else:\n",
    "            return self.vocab_source_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_encoded_target_word(self, word):\n",
    "        if word in self.vocab_target_lang:\n",
    "            return self.vocab_target_lang[word]\n",
    "        else:\n",
    "            return self.vocab_target_lang[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_decoded_target_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_target_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def get_decoded_source_word(self, index):\n",
    "        found = list(filter(lambda x: x[1] == index, self.vocab_source_lang.items()))\n",
    "        if len(found) > 0:\n",
    "            return found[0][0]\n",
    "        else:\n",
    "            return UNKNOWN_TOKEN\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': tensor([1432, 1221, 1358,  526,  562, 1089,  870, 1727, 1669,  302, 1934, 1020,\n",
      "          94, 1657, 2194, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  209,  908, 1551,  841,  843, 1370, 1030,  970, 1108,  700, 1398,\n",
      "        1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432,  316,  562,  671, 1392, 1643, 1238, 1438, 1657, 2194, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197, 1681, 1592, 1414, 1652, 1654, 1370, 1448,  761,  446, 1186,  794,\n",
      "        1398, 1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1238, 1730,  153,  415, 1669, 1238,   14,  215, 1513, 1657, 2194,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761,    3, 1050, 1753, 1016,  761, 1079,  746, 1398, 1849, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1238, 1069, 1669, 1567,   71,  405,  887, 2189, 2081, 1101, 2062,\n",
      "         293, 1238,  247, 1657, 2194, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761, 1572, 1414,  761, 1467, 1762, 1531, 1575, 1102,  761,   61,\n",
      "         554,  761,  399, 1398, 1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1221,  562, 1163, 1460, 1592, 2062, 1675,  689,  714, 1657, 2194,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  209, 1592, 1370,  621,  790,  328, 1305,   93, 1398, 1849, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1238, 1069, 1669,  782, 2034, 1518,  872, 1823, 1691,  302, 1793,\n",
      "        1069,  406,  405,  341, 1657, 2194, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761, 1572, 1414, 1744,  161,  761, 1523,  304,  790,  202, 1572,\n",
      "         428,  646, 1762, 1398, 1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1238, 1069, 1109, 2178,  989,  780,  280, 1657, 2194, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761, 1572, 1531, 1835,  621,  761, 1387, 1833, 1849, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1238, 1689,  153, 1032,  671, 1957, 2102, 1691,   79,  811, 1626,\n",
      "        1135, 1231, 1657, 2194, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761, 1246, 1050, 1042, 1102,  244, 1427,  304,  735, 1855,  796,\n",
      "         790, 1404, 1398, 1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1518,  761,  671, 2081,  460,   64, 1568,  280, 1567, 2155,  776,\n",
      "        1657, 2194, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197,  761,  868,  664,  761, 1043,  732, 1531, 1041, 1457,  761, 1759,\n",
      "        1398, 1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}, {'source': tensor([1432, 1348, 1355,  205, 1669,  302,  631, 2189,  660, 1657, 2194, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956,\n",
      "        1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]), 'target': tensor([1197, 1488, 1031, 1102,  662, 1414,  790, 1048,  346,  790,  343, 1398,\n",
      "        1849, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663,\n",
      "        1663, 1663, 1663])}]\n"
     ]
    }
   ],
   "source": [
    "dataset = MTDataset('data/multi30k_dev.txt')\n",
    "print(dataset[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def split_data(source_path, target_path_train, target_path_test, train_split=0.8):\n",
    "    with open(source_path, 'r') as source:\n",
    "        lines = source.readlines()\n",
    "\n",
    "    delimiter = int(len(lines) * train_split)\n",
    "\n",
    "    with open(target_path_train, 'w') as target_train:\n",
    "        for line in lines[:delimiter]:\n",
    "            target_train.write(line)\n",
    "    with open(target_path_test, 'w') as target_test:\n",
    "        for line in lines[delimiter:]:\n",
    "            target_test.write(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "split_data('data/multi30k_dev.txt', 'data/dev_train', 'data/dev_test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def dataloader(path_train, path_test, batch_size):\n",
    "    train_dataset = MTDataset(path_train, max_lines=-1)\n",
    "    test_dataset = MTDataset(path_test, max_lines=-1, dataset=train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataloader('data/dev_train', 'data/dev_test', hyperparameters['batch_size'])\n",
    "train_dataset = train_dataloader.dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 - Models\n",
    "### 2.1 - recurrent neural network (LSTM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class DCEPEncoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dim, encoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(source_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, encoder_out_dim, 8, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, source):\n",
    "        embedding = self.embeddings(source)\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        _, states = self.lstm(dropped_out)\n",
    "\n",
    "        return states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class DCEPDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, decoder_out_dim, padding_idx, dropout_prob):\n",
    "        super(DCEPDecoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(target_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, decoder_out_dim, 8, batch_first=True)\n",
    "        self.classifier = nn.Linear(decoder_out_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, target_word, input_states):\n",
    "        embedding = self.embeddings(target_word.unsqueeze(0).transpose(0,1))\n",
    "        dropped_out = self.dropout(embedding)\n",
    "        output, output_states = self.lstm(dropped_out, input_states)\n",
    "        prediction = self.classifier(output).squeeze(1)\n",
    "\n",
    "        return prediction, output_states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class DCEPSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoded_target_SOS, encoded_target_EOS):\n",
    "        super(DCEPSeq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoded_target_SOS = encoded_target_SOS\n",
    "        self.encoded_target_EOS = encoded_target_EOS\n",
    "\n",
    "    def forward(self, source, target=None):\n",
    "        predicted_sentence = []\n",
    "\n",
    "        states = self.encoder(source)\n",
    "\n",
    "        if target is not None:\n",
    "            target = target.transpose(0,1)\n",
    "            predicted_word = target[0]\n",
    "            for word in target:\n",
    "                base_word = word if random.random() > 1 else predicted_word\n",
    "\n",
    "                predicted_word_layer, states = self.decoder(base_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word_layer)\n",
    "        else:\n",
    "            sentence_length = 0\n",
    "            predicted_word = torch.tensor([self.encoded_target_SOS] * source.shape[0], device=device)\n",
    "            while predicted_word != torch.tensor(self.encoded_target_EOS, device=device):\n",
    "                predicted_word_layer, states = self.decoder(predicted_word, states)\n",
    "                predicted_word = torch.max(predicted_word_layer, 1).indices\n",
    "                predicted_sentence.append(predicted_word)\n",
    "                sentence_length += 1\n",
    "\n",
    "                if sentence_length > 30:\n",
    "                    break\n",
    "        return torch.stack(predicted_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index=train_dataset.get_encoded_target_word(PADDING_TOKEN))\n",
    "\n",
    "dcepEncoder = DCEPEncoder(len(train_dataset.vocab_source_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_source_word(PADDING_TOKEN),\n",
    "                          0)\n",
    "\n",
    "dcepDecoder = DCEPDecoder(len(train_dataset.vocab_target_lang),\n",
    "                          hyperparameters['embedding_dim'],\n",
    "                          hyperparameters['lstm_out_dim'],\n",
    "                          train_dataset.get_encoded_target_word(PADDING_TOKEN),\n",
    "                          0)\n",
    "dcepSeq2seq = DCEPSeq2Seq(dcepEncoder,\n",
    "                          dcepDecoder,\n",
    "                          train_dataset.get_encoded_target_word(START_TOKEN),\n",
    "                          train_dataset.get_encoded_target_word(END_TOKEN))\n",
    "dcepSeq2seq.to(device)\n",
    "\n",
    "optimizer = optim.Adam(dcepSeq2seq.parameters(), lr=hyperparameters['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def translate_test():\n",
    "    sentence = \"<SOS> Jungen tanzen mitten in der Nacht auf Pfosten .\".split(' ')\n",
    "    encoded_sentence = torch.tensor([train_dataset.get_encoded_source_word(word.lower()) for word in sentence], device=device).unsqueeze(0)\n",
    "    translated = dcepSeq2seq(encoded_sentence).squeeze(0)\n",
    "    return [train_dataset.get_decoded_target_word(int(word)) for word in translated]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 EPOCHS - 40 BATCHES PER EPOCH\n",
      "epoch 0, batch 39: 5.7259\n",
      "['a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 1, batch 39: 5.0754\n",
      "['a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 2, batch 39: 4.9849\n",
      "['a', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "epoch 3, batch 39: 4.9089\n",
      "['a', 'man', 'a', 'a', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '<EOS>']\n",
      "epoch 4, batch 39: 4.8579\n",
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(f'{hyperparameters[\"epochs\"]} EPOCHS - {math.floor(len(train_dataset) / train_dataloader.batch_size)} BATCHES PER EPOCH')\n",
    "\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        source = batch['source'].to(device)\n",
    "        target = batch['target'].type(torch.LongTensor).to(device)\n",
    "\n",
    "        output = dcepSeq2seq(source, target)\n",
    "#        print(output.transpose(0,1).size())\n",
    "#        print(target.size())\n",
    "#        print(torch.max(output.transpose(0,1), 2).indices.size())\n",
    "#        print()\n",
    "#        print([train_dataset.get_decoded_source_word(int(word)) for word in source[0]])\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in target[:, 1:][0]])\n",
    "#        max_output = torch.max(output.transpose(0,1)[:, :-1], 2).indices\n",
    "#        print([train_dataset.get_decoded_target_word(int(word)) for word in max_output[0]])\n",
    "        loss = loss_function(output.transpose(0,1)[:, :-1].reshape(-1, output.shape[2]), target[:, 1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        sys.stdout.write(f'\\repoch {epoch}, batch {i}: {np.round(total_loss / (i + 1), 4)}')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()\n",
    "    print(translate_test())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'in', 'a', 'a', 'a', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "dcepSeq2seq.eval()\n",
    "print(translate_test())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 - transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 - convolutional neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 - Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 - Discussion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "[1] Gehring et al. 2017. Convolutional Sequence to Sequence Learning\n",
    "[2] Gehring et al. 2017. A Convolutional Encoder Model for Neural Machine Translation\n",
    "[3] Cho et al. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "[4] Zhou et al. 2016. Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
    "[5] Zhou et al. 2020. Incorporating BERT into Neural Machine Translation\n",
    "[6] Vaswani et al. 2017. Attention is All you Need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}