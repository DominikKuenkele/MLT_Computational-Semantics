{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "device = torch.device('cpu')\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formatting data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to load some data, you can download the file on Canvas under files/assignments/03-lab-data/wiki-corpus.50000.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "data_path = 'data/'\n",
    "WINDOW_SIZE = 4\n",
    "OCCURRENCE_THRESHOLD = 1\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "\n",
    "\n",
    "def corpus_reader(data_path):\n",
    "    before = math.floor(WINDOW_SIZE / 2)\n",
    "    after = WINDOW_SIZE - before\n",
    "\n",
    "    samples = []\n",
    "    with open(data_path, encoding='UTF-8') as f:\n",
    "        lines = f.readlines()[:10]\n",
    "        occurrences = {}\n",
    "        for line in lines:\n",
    "            tokenized = [word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "            for word in tokenized:\n",
    "                occurrences[word] = occurrences.get(word, 0) + 1\n",
    "\n",
    "        unknown_words = [word for word in occurrences if occurrences[word] <= OCCURRENCE_THRESHOLD]\n",
    "\n",
    "        for line_index, line in enumerate(lines):\n",
    "            tokenized = [UNKNOWN_TOKEN if word in unknown_words else word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "            for index, word in enumerate(tokenized):\n",
    "                if word != UNKNOWN_TOKEN:\n",
    "                    context = []\n",
    "                    for i in range(index - before, index):\n",
    "                        if i >= 0:\n",
    "                            context.append(tokenized[i])\n",
    "                    for i in range(index + 1, index + after + 1):\n",
    "                        if i < len(tokenized):\n",
    "                            context.append(tokenized[i])\n",
    "                    samples.append((word, context))\n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#print(corpus_reader(data_path + 'wiki-corpus.50000.txt')[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good:\n",
    "- different subjects\n",
    "- big vocabulary\n",
    "- words are used in 'true' meaning (not metaphorical) grammatically because of informative nature\n",
    "\n",
    "### Bad:\n",
    "- one type of language\n",
    "- informative\n",
    "- may lose variety that can be in different kinds of texts\n",
    "- depending on goal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.vocab = {}\n",
    "        for target_word, context in data:\n",
    "            if target_word not in self.vocab:\n",
    "                self.vocab[target_word] = len(self.vocab)\n",
    "            for word in context:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = len(self.vocab)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context = self.data[idx]\n",
    "\n",
    "        encoded_context = [self.vocab[word] for word in context]\n",
    "        encoded_context.extend([len(encoded_context)] * (WINDOW_SIZE - len(encoded_context)))\n",
    "\n",
    "        Sample = namedtuple('Sample', 'target_word context')\n",
    "\n",
    "        return Sample(self.vocab[target_word], encoded_context)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def getVocab(self):\n",
    "        return self.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_data(path, batch_size):\n",
    "    samples = corpus_reader(path)\n",
    "    dataset = CustDataset(samples)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: x)\n",
    "\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good\n",
    "- beginning of sentences\n",
    "- no duplicates\n",
    "\n",
    "### Bad\n",
    "- some proper nouns may be missed, those that may have a counterpart in a common noun and vice versa (e.g. Mark/mark)\n",
    "- can create problems in other languages (e.g. German)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, dimensions)\n",
    "        self.prediction = nn.Linear(dimensions, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        projection = self.projection_function(embedded_context)\n",
    "        predictions = self.prediction(projection)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs, dim=1)\n",
    "        return xs_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs': 3,\n",
    "                                   'batch_size': 8,\n",
    "                                   'embedding_size': 128,\n",
    "                                   'learning_rate': 0.001}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "dataloader = get_data(data_path + 'wiki-corpus.50000.txt', word_embeddings_hyperparameters['batch_size'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "cbow_model = CBOWModel(len(dataloader.dataset), word_embeddings_hyperparameters['embedding_size'])\n",
    "torch.cuda.empty_cache()\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch count: 21\n",
      "epoch 0, batch 21: 5.3045\r\n",
      "batch count: 21\n",
      "epoch 1, batch 21: 3.8698\r\n",
      "batch count: 21\n",
      "epoch 2, batch 21: 3.2025\r\n",
      "CPU times: user 705 ms, sys: 15.2 ms, total: 721 ms\n",
      "Wall time: 385 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    print(f'batch count: {math.floor(len(dataloader.dataset) / dataloader.batch_size)}')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        contexts = torch.tensor([sample.context for sample in batch])\n",
    "        target_words = torch.tensor([sample.target_word for sample in batch])\n",
    "\n",
    "        contexts = contexts.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(contexts)\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(output.view(-1, len(dataloader.dataset)), target_words.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(f'epoch {epoch},', f'batch {i}:', np.round(total_loss / (i + 1), 4), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan]\n",
      " [nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/anaconda3/envs/computational-semantics/lib/python3.9/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/dominik/anaconda3/envs/computational-semantics/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/home/dominik/anaconda3/envs/computational-semantics/lib/python3.9/site-packages/numpy/lib/function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/home/dominik/anaconda3/envs/computational-semantics/lib/python3.9/site-packages/numpy/lib/function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/home/dominik/anaconda3/envs/computational-semantics/lib/python3.9/site-packages/numpy/lib/function_base.py:2542: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    count = 0\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "\n",
    "            score = float(score)\n",
    "\n",
    "            # get the index for the word\n",
    "            if word1 in vocab and word2 in vocab:\n",
    "                count += 1\n",
    "                print(count, end='\\r')\n",
    "                dataset_sims.append(score)\n",
    "                word1_idx = vocab[word1]\n",
    "                word2_idx = vocab[word2]\n",
    "\n",
    "                # get the embedding of the word\n",
    "                word1_emb = embeddings.weight[word1_idx]\n",
    "                word2_emb = embeddings.weight[word2_idx]\n",
    "                # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "                # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "                cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "\n",
    "                model_sims.append(cosine_similarity.item())\n",
    "\n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(data_path + path, dataloader.dataset.getVocab(), cbow_model.embeddings)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graveyard\n",
      "buck\n",
      "stove\n",
      "rook\n",
      "yen\n",
      "jaguar\n",
      "brandy\n",
      "thunderstorm\n",
      "feline\n",
      "gem\n",
      "recess\n",
      "madhouse\n",
      "carnivore\n",
      "277\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    words = set()\n",
    "    with open('data/wordsim_similarity_goldstandard.txt') as f:\n",
    "        for line in f:\n",
    "            words_gold = line.split('\\t')\n",
    "            words.add(words_gold[0])\n",
    "            words.add(words_gold[1])\n",
    "\n",
    "    words_wiki = set()\n",
    "    with open('data/wiki-corpus.50000.txt') as f:\n",
    "        for line in f:\n",
    "            words_tmp = line.split(' ')\n",
    "            words_wiki.update(words_tmp)\n",
    "\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_wiki:\n",
    "            print(word)\n",
    "    print(len(words))\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "#### full wiki-50000\n",
    "```\n",
    "Epoch 0: 7.5331\n",
    "Epoch 1: 7.1656\n",
    "Epoch 2: 7.0064\n",
    "CPU times: user 11h 28min 53s, sys: 6h 9min 52s, total: 17h 38min 45s\n",
    "Wall time: 17h 46min 41s\n",
    "\n",
    "[[1.         0.22844377]\n",
    " [0.22844377 1.        ]]\n",
    "```\n",
    "#### wiki-50000 (min-freq 1)\n",
    "```\n",
    "batch number: 155520.125\n",
    "epoch 0, batch 155520: 7.2749\n",
    "batch number: 155520.125\n",
    "epoch 1, batch 155520: 6.8959\n",
    "batch number: 155520.125\n",
    "epoch 2, batch 155520: 6.7349\n",
    "CPU times: user 11h 53min 38s, sys: 6h 1min 25s, total: 17h 55min 3s\n",
    "Wall time: 17h 58min 4s\n",
    "\n",
    "[[1.         0.15954327]\n",
    " [0.15954327 1.        ]]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Training takes a long time\n",
    "- looking on pearson correlation, the model performs not to good, but still finds similarities\n",
    "- there are 13 words of the gold standard, which do not appear in the wiki-corpus-50000. This lowers the expressiveness of the pearson correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- use larger corpus\n",
    "- change hyperparameters (more epochs, higher learning rate, ...)\n",
    "- refine model (add dropout, ...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-corpus.50000.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs': 3,\n",
    "                      'batch_size': 8,\n",
    "                      'learning_rate': 0.001,\n",
    "                      'embedding_dim': 128,\n",
    "                      'output_dim': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "OCCURRENCE_THRESHOLD = 1\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        samples = []\n",
    "        with open(file, encoding='UTF-8') as f:\n",
    "            lines = f.readlines()[:10000]\n",
    "            occurrences = {}\n",
    "            for line in lines:\n",
    "                tokenized = [word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "                for word in tokenized:\n",
    "                    occurrences[word] = occurrences.get(word, 0) + 1\n",
    "\n",
    "            unknown_words = [word for word in occurrences if occurrences[word] <= OCCURRENCE_THRESHOLD]\n",
    "\n",
    "            for line in lines:\n",
    "                tokenized = [UNKNOWN_TOKEN if word in unknown_words else word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "                samples.append(tokenized)\n",
    "\n",
    "\n",
    "        vocab_list = [PADDING_TOKEN, *list(occurrences.keys())]\n",
    "        vocab_list.extend([UNKNOWN_TOKEN, START_TOKEN, END_TOKEN])\n",
    "        self.vocab = {word: index for index, word in enumerate(vocab_list)}\n",
    "        self.data = samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        encoded_sentence = [self.vocab[word] for word in sentence]\n",
    "        Sample = namedtuple('Sample', 'train eval')\n",
    "\n",
    "        train = [self.vocab[START_TOKEN], *encoded_sentence]\n",
    "        encoded_sentence.append(self.vocab[END_TOKEN])\n",
    "        return Sample(torch.tensor(train), torch.tensor(encoded_sentence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def getVocab(self):\n",
    "        return self.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "Sample(train=tensor([31231,    31,    36,    37,    38,    39,    40,    41,    42,    43,\n            8,    44,    11,     8,    45,    24,    46,    40,    47,    35]), eval=tensor([   31,    36,    37,    38,    39,    40,    41,    42,    43,     8,\n           44,    11,     8,    45,    24,    46,    40,    47,    35, 31232]))"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LMDataset('data/wiki-corpus.50000.txt')\n",
    "dataset[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "def get_lm_data(path, batch_size):\n",
    "    dataset = LMDataset(path)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: x)\n",
    "\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions, out_dim, num_layers):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings =  nn.Embedding(vocab_size, dimensions)\n",
    "        self.LSTM = nn.LSTM(dimensions, out_dim, num_layers=num_layers)\n",
    "        self.predict_word = nn.Linear(out_dim, vocab_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_reprentation, *_ = self.LSTM(embedded_seq)\n",
    "        predicted_words = self.predict_word(timestep_reprentation)\n",
    "\n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch count: 1250\n",
      "epoch 0, batch 1138: 10.3535\rtch 101: 10.3525epoch 0, batch 103: 10.3525epoch 0, batch 105: 10.3525epoch 0, batch 108: 10.3525epoch 0, batch 111: 10.3525epoch 0, batch 113: 10.3526epoch 0, batch 115: 10.3526epoch 0, batch 117: 10.3527epoch 0, batch 120: 10.3527epoch 0, batch 122: 10.3527epoch 0, batch 124: 10.3526epoch 0, batch 126: 10.3526epoch 0, batch 128: 10.3527epoch 0, batch 130: 10.3527epoch 0, batch 132: 10.3527epoch 0, batch 134: 10.3527epoch 0, batch 136: 10.3528epoch 0, batch 138: 10.3528epoch 0, batch 140: 10.3528epoch 0, batch 142: 10.3528epoch 0, batch 144: 10.3528epoch 0, batch 146: 10.3528epoch 0, batch 148: 10.3528epoch 0, batch 150: 10.3527epoch 0, batch 152: 10.3527epoch 0, batch 154: 10.3528epoch 0, batch 157: 10.3528epoch 0, batch 159: 10.3527epoch 0, batch 161: 10.3527epoch 0, batch 163: 10.3527epoch 0, batch 165: 10.3527epoch 0, batch 167: 10.3527epoch 0, batch 169: 10.3527epoch 0, batch 171: 10.3527epoch 0, batch 173: 10.3527epoch 0, batch 176: 10.3527epoch 0, batch 178: 10.3527epoch 0, batch 180: 10.3527epoch 0, batch 182: 10.3526epoch 0, batch 184: 10.3526epoch 0, batch 186: 10.3527epoch 0, batch 188: 10.3527epoch 0, batch 190: 10.3527epoch 0, batch 192: 10.3527epoch 0, batch 194: 10.3528epoch 0, batch 196: 10.3528epoch 0, batch 198: 10.3528epoch 0, batch 200: 10.3528epoch 0, batch 202: 10.3529epoch 0, batch 204: 10.3529epoch 0, batch 207: 10.3529epoch 0, batch 209: 10.3529epoch 0, batch 211: 10.3528epoch 0, batch 213: 10.3528epoch 0, batch 215: 10.3528epoch 0, batch 217: 10.3528epoch 0, batch 219: 10.3528epoch 0, batch 222: 10.3529epoch 0, batch 224: 10.3528epoch 0, batch 226: 10.3528epoch 0, batch 228: 10.3529epoch 0, batch 230: 10.3529epoch 0, batch 232: 10.353epoch 0, batch 234: 10.353epoch 0, batch 236: 10.353epoch 0, batch 238: 10.353epoch 0, batch 240: 10.353epoch 0, batch 242: 10.3529epoch 0, batch 245: 10.3529epoch 0, batch 247: 10.3529epoch 0, batch 249: 10.3529epoch 0, batch 251: 10.3529epoch 0, batch 253: 10.3529epoch 0, batch 255: 10.3529epoch 0, batch 257: 10.3529epoch 0, batch 259: 10.3529epoch 0, batch 261: 10.3529epoch 0, batch 263: 10.3529epoch 0, batch 265: 10.3529epoch 0, batch 267: 10.3529epoch 0, batch 269: 10.3529epoch 0, batch 271: 10.3529epoch 0, batch 273: 10.3529epoch 0, batch 275: 10.3529epoch 0, batch 277: 10.3529epoch 0, batch 279: 10.3529epoch 0, batch 281: 10.3528epoch 0, batch 283: 10.3528epoch 0, batch 285: 10.3528epoch 0, batch 287: 10.3528epoch 0, batch 289: 10.3528epoch 0, batch 291: 10.3528epoch 0, batch 293: 10.3529epoch 0, batch 295: 10.3529epoch 0, batch 297: 10.3529epoch 0, batch 299: 10.353epoch 0, batch 301: 10.353epoch 0, batch 303: 10.353epoch 0, batch 305: 10.353epoch 0, batch 307: 10.353epoch 0, batch 309: 10.353epoch 0, batch 311: 10.353epoch 0, batch 313: 10.353epoch 0, batch 315: 10.353epoch 0, batch 317: 10.353epoch 0, batch 319: 10.353epoch 0, batch 321: 10.353epoch 0, batch 323: 10.353epoch 0, batch 325: 10.353epoch 0, batch 328: 10.353epoch 0, batch 330: 10.353epoch 0, batch 333: 10.353epoch 0, batch 335: 10.353epoch 0, batch 337: 10.353epoch 0, batch 339: 10.353epoch 0, batch 341: 10.353epoch 0, batch 343: 10.353epoch 0, batch 345: 10.353epoch 0, batch 347: 10.353epoch 0, batch 349: 10.353epoch 0, batch 351: 10.353epoch 0, batch 353: 10.353epoch 0, batch 355: 10.353epoch 0, batch 357: 10.353epoch 0, batch 359: 10.3531epoch 0, batch 361: 10.3531epoch 0, batch 363: 10.3531epoch 0, batch 365: 10.3531epoch 0, batch 367: 10.3531epoch 0, batch 369: 10.3531epoch 0, batch 371: 10.3531epoch 0, batch 373: 10.353epoch 0, batch 375: 10.353epoch 0, batch 377: 10.353epoch 0, batch 379: 10.353epoch 0, batch 381: 10.353epoch 0, batch 383: 10.353epoch 0, batch 385: 10.353epoch 0, batch 387: 10.3531epoch 0, batch 389: 10.3531epoch 0, batch 391: 10.3531epoch 0, batch 393: 10.3531epoch 0, batch 395: 10.3531epoch 0, batch 397: 10.3531epoch 0, batch 399: 10.3531epoch 0, batch 401: 10.3531epoch 0, batch 403: 10.3531epoch 0, batch 405: 10.3531epoch 0, batch 407: 10.3531epoch 0, batch 409: 10.3531epoch 0, batch 411: 10.3531epoch 0, batch 413: 10.3531epoch 0, batch 415: 10.3531epoch 0, batch 417: 10.3531epoch 0, batch 419: 10.3531epoch 0, batch 421: 10.3531epoch 0, batch 423: 10.3531epoch 0, batch 425: 10.3531epoch 0, batch 427: 10.3531epoch 0, batch 429: 10.3531epoch 0, batch 431: 10.3531epoch 0, batch 433: 10.3531epoch 0, batch 435: 10.3531epoch 0, batch 437: 10.3531epoch 0, batch 439: 10.3531epoch 0, batch 441: 10.3531epoch 0, batch 443: 10.3531epoch 0, batch 445: 10.3531epoch 0, batch 447: 10.3531epoch 0, batch 449: 10.3531epoch 0, batch 451: 10.3531epoch 0, batch 453: 10.3531epoch 0, batch 456: 10.3531epoch 0, batch 458: 10.3531epoch 0, batch 460: 10.3531epoch 0, batch 462: 10.3531epoch 0, batch 464: 10.3531epoch 0, batch 466: 10.3531epoch 0, batch 468: 10.3531epoch 0, batch 470: 10.3531epoch 0, batch 472: 10.3531epoch 0, batch 474: 10.3531epoch 0, batch 476: 10.3532epoch 0, batch 478: 10.3532epoch 0, batch 480: 10.3531epoch 0, batch 482: 10.3532epoch 0, batch 484: 10.3532epoch 0, batch 486: 10.3532epoch 0, batch 488: 10.3532epoch 0, batch 490: 10.3532epoch 0, batch 492: 10.3532epoch 0, batch 494: 10.3532epoch 0, batch 496: 10.3532epoch 0, batch 498: 10.3532epoch 0, batch 500: 10.3532epoch 0, batch 502: 10.3532epoch 0, batch 504: 10.3532epoch 0, batch 506: 10.3533epoch 0, batch 508: 10.3533epoch 0, batch 510: 10.3532epoch 0, batch 512: 10.3533epoch 0, batch 514: 10.3533epoch 0, batch 516: 10.3533epoch 0, batch 518: 10.3533epoch 0, batch 520: 10.3533epoch 0, batch 522: 10.3533epoch 0, batch 524: 10.3533epoch 0, batch 526: 10.3533epoch 0, batch 528: 10.3533epoch 0, batch 530: 10.3533epoch 0, batch 532: 10.3533epoch 0, batch 534: 10.3533epoch 0, batch 536: 10.3533epoch 0, batch 538: 10.3533epoch 0, batch 540: 10.3533epoch 0, batch 542: 10.3533epoch 0, batch 544: 10.3533epoch 0, batch 546: 10.3533epoch 0, batch 548: 10.3533epoch 0, batch 550: 10.3533epoch 0, batch 552: 10.3533epoch 0, batch 554: 10.3533epoch 0, batch 556: 10.3533epoch 0, batch 558: 10.3533epoch 0, batch 560: 10.3533epoch 0, batch 562: 10.3533epoch 0, batch 564: 10.3533epoch 0, batch 566: 10.3533epoch 0, batch 568: 10.3533epoch 0, batch 570: 10.3533epoch 0, batch 572: 10.3533epoch 0, batch 574: 10.3533epoch 0, batch 576: 10.3533epoch 0, batch 578: 10.3533epoch 0, batch 580: 10.3533epoch 0, batch 582: 10.3533epoch 0, batch 584: 10.3533epoch 0, batch 586: 10.3533epoch 0, batch 588: 10.3533epoch 0, batch 590: 10.3533epoch 0, batch 592: 10.3533epoch 0, batch 594: 10.3533epoch 0, batch 596: 10.3533epoch 0, batch 598: 10.3533epoch 0, batch 600: 10.3533epoch 0, batch 602: 10.3533epoch 0, batch 604: 10.3533epoch 0, batch 606: 10.3533epoch 0, batch 608: 10.3533epoch 0, batch 610: 10.3533epoch 0, batch 612: 10.3533epoch 0, batch 614: 10.3533epoch 0, batch 616: 10.3533epoch 0, batch 618: 10.3533epoch 0, batch 620: 10.3533epoch 0, batch 622: 10.3533epoch 0, batch 624: 10.3533epoch 0, batch 626: 10.3533epoch 0, batch 628: 10.3533epoch 0, batch 630: 10.3533epoch 0, batch 632: 10.3533epoch 0, batch 634: 10.3533epoch 0, batch 636: 10.3533epoch 0, batch 638: 10.3534epoch 0, batch 640: 10.3534epoch 0, batch 642: 10.3534epoch 0, batch 644: 10.3534epoch 0, batch 646: 10.3534epoch 0, batch 648: 10.3534epoch 0, batch 650: 10.3534epoch 0, batch 652: 10.3534epoch 0, batch 654: 10.3534epoch 0, batch 656: 10.3534epoch 0, batch 658: 10.3534epoch 0, batch 660: 10.3534epoch 0, batch 662: 10.3534epoch 0, batch 664: 10.3534epoch 0, batch 666: 10.3534epoch 0, batch 668: 10.3534epoch 0, batch 670: 10.3534epoch 0, batch 672: 10.3534epoch 0, batch 674: 10.3534epoch 0, batch 676: 10.3534epoch 0, batch 678: 10.3534epoch 0, batch 680: 10.3534epoch 0, batch 682: 10.3534epoch 0, batch 684: 10.3534epoch 0, batch 686: 10.3534epoch 0, batch 689: 10.3534epoch 0, batch 691: 10.3534epoch 0, batch 693: 10.3534epoch 0, batch 695: 10.3534epoch 0, batch 697: 10.3534epoch 0, batch 699: 10.3534epoch 0, batch 701: 10.3534epoch 0, batch 703: 10.3535epoch 0, batch 705: 10.3535epoch 0, batch 707: 10.3535epoch 0, batch 709: 10.3535epoch 0, batch 711: 10.3535epoch 0, batch 713: 10.3535epoch 0, batch 715: 10.3535epoch 0, batch 717: 10.3535epoch 0, batch 719: 10.3535epoch 0, batch 721: 10.3535epoch 0, batch 723: 10.3535epoch 0, batch 725: 10.3535epoch 0, batch 727: 10.3535epoch 0, batch 729: 10.3535epoch 0, batch 731: 10.3535epoch 0, batch 733: 10.3535epoch 0, batch 735: 10.3535epoch 0, batch 737: 10.3535epoch 0, batch 739: 10.3535epoch 0, batch 741: 10.3535epoch 0, batch 743: 10.3535epoch 0, batch 745: 10.3535epoch 0, batch 747: 10.3535epoch 0, batch 749: 10.3535epoch 0, batch 751: 10.3535epoch 0, batch 753: 10.3535epoch 0, batch 755: 10.3535epoch 0, batch 757: 10.3535epoch 0, batch 759: 10.3535epoch 0, batch 761: 10.3535epoch 0, batch 763: 10.3535epoch 0, batch 765: 10.3535epoch 0, batch 767: 10.3535epoch 0, batch 769: 10.3535epoch 0, batch 771: 10.3534epoch 0, batch 773: 10.3534epoch 0, batch 775: 10.3535epoch 0, batch 777: 10.3535epoch 0, batch 779: 10.3535epoch 0, batch 781: 10.3535epoch 0, batch 783: 10.3535epoch 0, batch 785: 10.3535epoch 0, batch 787: 10.3535epoch 0, batch 789: 10.3535epoch 0, batch 791: 10.3535epoch 0, batch 793: 10.3535epoch 0, batch 795: 10.3535epoch 0, batch 797: 10.3535epoch 0, batch 800: 10.3535epoch 0, batch 802: 10.3535epoch 0, batch 804: 10.3535epoch 0, batch 806: 10.3535epoch 0, batch 808: 10.3535epoch 0, batch 810: 10.3535epoch 0, batch 812: 10.3535epoch 0, batch 814: 10.3535epoch 0, batch 816: 10.3535epoch 0, batch 818: 10.3535epoch 0, batch 820: 10.3535epoch 0, batch 822: 10.3535epoch 0, batch 824: 10.3535epoch 0, batch 826: 10.3535epoch 0, batch 828: 10.3535epoch 0, batch 830: 10.3535epoch 0, batch 832: 10.3535epoch 0, batch 834: 10.3535epoch 0, batch 836: 10.3535epoch 0, batch 838: 10.3535epoch 0, batch 840: 10.3536epoch 0, batch 842: 10.3536epoch 0, batch 844: 10.3535epoch 0, batch 846: 10.3535epoch 0, batch 848: 10.3535epoch 0, batch 850: 10.3535epoch 0, batch 852: 10.3536epoch 0, batch 854: 10.3535epoch 0, batch 857: 10.3535epoch 0, batch 859: 10.3535epoch 0, batch 861: 10.3535epoch 0, batch 863: 10.3535epoch 0, batch 865: 10.3535epoch 0, batch 867: 10.3535epoch 0, batch 869: 10.3536epoch 0, batch 871: 10.3535epoch 0, batch 873: 10.3535epoch 0, batch 875: 10.3535epoch 0, batch 877: 10.3536epoch 0, batch 879: 10.3536epoch 0, batch 881: 10.3535epoch 0, batch 883: 10.3535epoch 0, batch 885: 10.3535epoch 0, batch 887: 10.3535epoch 0, batch 889: 10.3536epoch 0, batch 891: 10.3536epoch 0, batch 893: 10.3535epoch 0, batch 895: 10.3536epoch 0, batch 897: 10.3536epoch 0, batch 899: 10.3535epoch 0, batch 901: 10.3536epoch 0, batch 903: 10.3536epoch 0, batch 905: 10.3536epoch 0, batch 907: 10.3536epoch 0, batch 909: 10.3536epoch 0, batch 911: 10.3536epoch 0, batch 913: 10.3536epoch 0, batch 915: 10.3536epoch 0, batch 917: 10.3535epoch 0, batch 920: 10.3535epoch 0, batch 922: 10.3535epoch 0, batch 924: 10.3535epoch 0, batch 926: 10.3535epoch 0, batch 928: 10.3535epoch 0, batch 930: 10.3535epoch 0, batch 932: 10.3535epoch 0, batch 934: 10.3535epoch 0, batch 937: 10.3535epoch 0, batch 939: 10.3535epoch 0, batch 941: 10.3535epoch 0, batch 943: 10.3535epoch 0, batch 945: 10.3535epoch 0, batch 947: 10.3535epoch 0, batch 949: 10.3535epoch 0, batch 951: 10.3535epoch 0, batch 953: 10.3535epoch 0, batch 955: 10.3535epoch 0, batch 958: 10.3535epoch 0, batch 960: 10.3535epoch 0, batch 962: 10.3535epoch 0, batch 964: 10.3535epoch 0, batch 966: 10.3535epoch 0, batch 968: 10.3535epoch 0, batch 970: 10.3534epoch 0, batch 972: 10.3534epoch 0, batch 974: 10.3534epoch 0, batch 976: 10.3534epoch 0, batch 978: 10.3534epoch 0, batch 980: 10.3534epoch 0, batch 982: 10.3534epoch 0, batch 984: 10.3535epoch 0, batch 986: 10.3535epoch 0, batch 988: 10.3534epoch 0, batch 990: 10.3534epoch 0, batch 992: 10.3534epoch 0, batch 994: 10.3534epoch 0, batch 996: 10.3534epoch 0, batch 998: 10.3534epoch 0, batch 1000: 10.3534epoch 0, batch 1002: 10.3534epoch 0, batch 1004: 10.3534epoch 0, batch 1006: 10.3534epoch 0, batch 1008: 10.3534epoch 0, batch 1010: 10.3534epoch 0, batch 1012: 10.3534epoch 0, batch 1014: 10.3534epoch 0, batch 1016: 10.3534epoch 0, batch 1018: 10.3534epoch 0, batch 1020: 10.3534epoch 0, batch 1022: 10.3534epoch 0, batch 1024: 10.3534epoch 0, batch 1026: 10.3534epoch 0, batch 1028: 10.3534epoch 0, batch 1030: 10.3534epoch 0, batch 1032: 10.3534epoch 0, batch 1034: 10.3534epoch 0, batch 1036: 10.3534epoch 0, batch 1038: 10.3534epoch 0, batch 1040: 10.3534epoch 0, batch 1042: 10.3534epoch 0, batch 1044: 10.3534epoch 0, batch 1046: 10.3534epoch 0, batch 1048: 10.3534epoch 0, batch 1050: 10.3534epoch 0, batch 1052: 10.3534epoch 0, batch 1054: 10.3534epoch 0, batch 1056: 10.3534epoch 0, batch 1058: 10.3534epoch 0, batch 1060: 10.3535epoch 0, batch 1062: 10.3535epoch 0, batch 1064: 10.3535epoch 0, batch 1067: 10.3535epoch 0, batch 1069: 10.3535epoch 0, batch 1071: 10.3535epoch 0, batch 1073: 10.3535epoch 0, batch 1075: 10.3535epoch 0, batch 1077: 10.3535epoch 0, batch 1079: 10.3535epoch 0, batch 1081: 10.3535epoch 0, batch 1083: 10.3535epoch 0, batch 1085: 10.3535epoch 0, batch 1087: 10.3535epoch 0, batch 1089: 10.3535epoch 0, batch 1091: 10.3535epoch 0, batch 1093: 10.3535epoch 0, batch 1095: 10.3535epoch 0, batch 1097: 10.3535epoch 0, batch 1099: 10.3535epoch 0, batch 1101: 10.3535epoch 0, batch 1103: 10.3535epoch 0, batch 1105: 10.3535epoch 0, batch 1107: 10.3535epoch 0, batch 1109: 10.3535epoch 0, batch 1111: 10.3535epoch 0, batch 1113: 10.3535epoch 0, batch 1115: 10.3535epoch 0, batch 1117: 10.3535epoch 0, batch 1119: 10.3535epoch 0, batch 1121: 10.3535epoch 0, batch 1123: 10.3535epoch 0, batch 1125: 10.3535epoch 0, batch 1127: 10.3535epoch 0, batch 1129: 10.3535epoch 0, batch 1131: 10.3535epoch 0, batch 1133: 10.3535epoch 0, batch 1135: 10.3535epoch 0, batch 1137: 10.3535epoch 0, batch 1139: 10.3535\r"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# load data\n",
    "dataloader = get_lm_data(data_path + 'wiki-corpus.50000.txt', lm_hyperparameters['batch_size'])\n",
    "\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(dataloader.dataset.getVocab()),\n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'],\n",
    "                       1)\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    print(f'batch count: {math.floor(len(dataloader.dataset) / dataloader.batch_size)}')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "\n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = pad_sequence([sample.train for sample in batch])\n",
    "        gold_data = pad_sequence([sample.eval for sample in batch])\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "\n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "\n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        loss = loss_fn(output.view(-1, len(dataloader.dataset.getVocab())), gold_data.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(f'\\repoch {epoch},', f'batch {i}:', np.round(total_loss / (i + 1), 4), end='')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "\n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = ...\n",
    "            tok_bad_s = ...\n",
    "\n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([_ for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([_ for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "\n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = LM_withLSTM(enc_good_s)\n",
    "            bad_s = LM_withLSTM(enc_bad_s)\n",
    "\n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(...)\n",
    "            bs_probs = F.softmax(...)\n",
    "\n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "\n",
    "            accuracy.append(int(gs_sent_prob > bs_sent_prob))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = ...\n",
    "        probs.append(prob)\n",
    "    sentence_prob = ...\n",
    "    return sentence_prob\n",
    "\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, ..., ...)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}