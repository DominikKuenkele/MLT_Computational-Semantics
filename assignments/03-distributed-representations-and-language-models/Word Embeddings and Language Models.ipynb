{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "device = torch.device('cuda:2')\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "data_path = 'data/'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "PADDING_TOKEN = '<PAD>'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formatting data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to load some data, you can download the file on Canvas under files/assignments/03-lab-data/wiki-corpus.50000.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 4\n",
    "OCCURRENCE_THRESHOLD = 1\n",
    "\n",
    "\n",
    "def corpus_reader(data_path, number_sentences):\n",
    "    before = math.floor(WINDOW_SIZE / 2)\n",
    "    after = WINDOW_SIZE - before\n",
    "\n",
    "    samples = []\n",
    "    with open(data_path, encoding='UTF-8') as f:\n",
    "        lines = f.readlines()[:number_sentences]\n",
    "        occurrences = {}\n",
    "        for line in lines:\n",
    "            tokenized = [word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "            for word in tokenized:\n",
    "                occurrences[word] = occurrences.get(word, 0) + 1\n",
    "\n",
    "        unknown_words = [word for word in occurrences if occurrences[word] <= OCCURRENCE_THRESHOLD]\n",
    "\n",
    "        for line_index, line in enumerate(lines):\n",
    "            tokenized = [UNKNOWN_TOKEN if word in unknown_words else word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "            for index, word in enumerate(tokenized):\n",
    "                if word != UNKNOWN_TOKEN:\n",
    "                    context = []\n",
    "                    for i in range(index - before, index):\n",
    "                        if i >= 0:\n",
    "                            context.append(tokenized[i])\n",
    "                    for i in range(index + 1, index + after + 1):\n",
    "                        if i < len(tokenized):\n",
    "                            context.append(tokenized[i])\n",
    "                    samples.append((word, context))\n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(corpus_reader(data_path + 'wiki-corpus.50000.txt')[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good:\n",
    "- different subjects\n",
    "- big vocabulary\n",
    "- words are used in 'true' meaning (not metaphorical) grammatically because of informative nature\n",
    "\n",
    "### Bad:\n",
    "- one type of language\n",
    "- informative\n",
    "- may lose variety that can be in different kinds of texts\n",
    "- depending on goal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.vocab = {}\n",
    "        for target_word, context in data:\n",
    "            if target_word not in self.vocab:\n",
    "                self.vocab[target_word] = len(self.vocab)\n",
    "            for word in context:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = len(self.vocab)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context = self.data[idx]\n",
    "\n",
    "        encoded_context = [self.get_encoded_word(word) for word in context]\n",
    "        encoded_context.extend([len(encoded_context)] * (WINDOW_SIZE - len(encoded_context)))\n",
    "\n",
    "        Sample = namedtuple('Sample', 'target_word context')\n",
    "\n",
    "        return Sample(self.get_encoded_word(target_word), encoded_context)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_encoded_word(self, word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return self.vocab[UNKNOWN_TOKEN] "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_data(path, batch_size):\n",
    "    samples = corpus_reader(path, 10000)\n",
    "    dataset = CBOWDataset(samples)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: x)\n",
    "\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good\n",
    "- beginning of sentences\n",
    "- no duplicates\n",
    "\n",
    "### Bad\n",
    "- some proper nouns may be missed, those that may have a counterpart in a common noun and vice versa (e.g. Mark/mark)\n",
    "- can create problems in other languages (e.g. German)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, dimensions)\n",
    "        self.prediction = nn.Linear(dimensions, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        projection = self.projection_function(embedded_context)\n",
    "        predictions = self.prediction(projection)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs, dim=1)\n",
    "        return xs_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs': 3,\n",
    "                                   'batch_size': 8,\n",
    "                                   'embedding_size': 128,\n",
    "                                   'learning_rate': 0.001}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cbow_dataloader = get_data(data_path + 'wiki-corpus.50000.txt', word_embeddings_hyperparameters['batch_size'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cbow_model = CBOWModel(len(cbow_dataloader.dataset), word_embeddings_hyperparameters['embedding_size'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    print(f'batch count: {math.floor(len(cbow_dataloader.dataset) / cbow_dataloader.batch_size)}')\n",
    "    for i, batch in enumerate(cbow_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        contexts = torch.tensor([sample.context for sample in batch], device=device)\n",
    "        target_words = torch.tensor([sample.target_word for sample in batch], device=device)\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(contexts)\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(output.view(-1, len(cbow_dataloader.dataset)), target_words.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(f'epoch {epoch},', f'batch {i}:', np.round(total_loss / (i + 1), 4), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        del contexts\n",
    "        del target_words\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, dataset, embeddings):\n",
    "    count = 0\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "\n",
    "            score = float(score)\n",
    "\n",
    "            # get the index for the word\n",
    "            count += 1\n",
    "            print(count, end='\\r')\n",
    "            dataset_sims.append(score)\n",
    "            word1_idx = dataset.get_encoded_word(word1)\n",
    "            word2_idx = dataset.get_encoded_word(word2)\n",
    "\n",
    "            # get the embedding of the word\n",
    "            word1_emb = embeddings.weight[word1_idx]\n",
    "            word2_emb = embeddings.weight[word2_idx]\n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "\n",
    "            model_sims.append(cosine_similarity.item())\n",
    "\n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(data_path + path, cbow_dataloader.dataset, cbow_model.embeddings)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "#### full wiki-50000\n",
    "```\n",
    "Epoch 0: 7.5331\n",
    "Epoch 1: 7.1656\n",
    "Epoch 2: 7.0064\n",
    "CPU times: user 11h 28min 53s, sys: 6h 9min 52s, total: 17h 38min 45s\n",
    "Wall time: 17h 46min 41s\n",
    "\n",
    "[[1.         0.22844377]\n",
    " [0.22844377 1.        ]]\n",
    "```\n",
    "#### wiki-50000 (min-freq 1)\n",
    "```\n",
    "batch number: 155520.125\n",
    "epoch 0, batch 155520: 7.2749\n",
    "batch number: 155520.125\n",
    "epoch 1, batch 155520: 6.8959\n",
    "batch number: 155520.125\n",
    "epoch 2, batch 155520: 6.7349\n",
    "CPU times: user 11h 53min 38s, sys: 6h 1min 25s, total: 17h 55min 3s\n",
    "Wall time: 17h 58min 4s\n",
    "\n",
    "[[1.         0.15954327]\n",
    " [0.15954327 1.        ]]\n",
    "```\n",
    "\n",
    "#### wiki-50000 (min-freq=1, sentences=10000)\n",
    "```\n",
    "batch count: 33071\n",
    "epoch 0, batch 33071: 7.6517\n",
    "batch count: 33071\n",
    "epoch 1, batch 33071: 6.7243\n",
    "batch count: 33071\n",
    "epoch 2, batch 33071: 6.4683\n",
    "CPU times: user 42min 29s, sys: 15min 27s, total: 57min 57s\n",
    "Wall time: 59min 12s\n",
    "\n",
    "[[1.         0.20549521]\n",
    " [0.20549521 1.        ]]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('cbow.pickle', 'wb') as f:\n",
    "    pickle.dump((cbow_model, cbow_dataloader), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('cbow.pickle', 'rb') as f:\n",
    "    cbow_model, cbow_dataloader = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Training takes a long time\n",
    "- looking on pearson correlation, the model performs not to good, but still finds similarities\n",
    "- there are 13 words of the gold standard, which do not appear in the wiki-corpus-50000. This lowers the expressiveness of the pearson correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "similarities = []\n",
    "\n",
    "with open(data_path + 'wordsim_similarity_goldstandard.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for index, line in enumerate(lines):\n",
    "        word1, word2, _ = line.rstrip('\\n').split()\n",
    "        similarities.append(((word1, word2), model[index]))\n",
    "\n",
    "similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Best 10:')\n",
    "pprint(similarities[:10])\n",
    "print()\n",
    "print('Worst 10:')\n",
    "pprint(similarities[-10:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results:\n",
    "```\n",
    "Best 10:\n",
    "[(('tiger', 'tiger'), 1.0),\n",
    " (('Arafat', 'Jackson'), 1.0),\n",
    " (('vodka', 'brandy'), 1.0),\n",
    " (('gem', 'jewel'), 1.0),\n",
    " (('tiger', 'jaguar'), 1.0),\n",
    " (('tiger', 'feline'), 1.0),\n",
    " (('tiger', 'carnivore'), 1.0),\n",
    " (('Japanese', 'American'), 1.0),\n",
    " (('Harvard', 'Yale'), 1.0),\n",
    " (('marathon', 'sprint'), 1.0)]\n",
    "\n",
    "Worst 10:\n",
    "[(('doctor', 'personnel'), -0.125071719288826),\n",
    " (('rock', 'jazz'), -0.12563274800777435),\n",
    " (('football', 'soccer'), -0.13329313695430756),\n",
    " (('cucumber', 'potato'), -0.14495040476322174),\n",
    " (('seven', 'series'), -0.14708052575588226),\n",
    " (('dollar', 'yen'), -0.1481156051158905),\n",
    " (('physics', 'chemistry'), -0.15008291602134705),\n",
    " (('cup', 'artifact'), -0.15044575929641724),\n",
    " (('cup', 'tableware'), -0.16372840106487274),\n",
    " (('professor', 'doctor'), -0.1771547794342041)]\n",
    "```\n",
    "\n",
    "The best 10 pairs make sense in our view. Also the worst 10 make sense on their own (e.g. jazz/rock are 'opposite' genres and perform bad). But the relations between the word pairs are different for best and worst (e.g. vodka/brandy are 'opposite' types of alcohol and perform good). The same applies to 'Japanese' and 'American', which has a similarity of 1, while 'dollar' and 'yen' are not similar at all, even though the relation is very similar.\n",
    "\n",
    "On the other side, it is striking that proper nouns appear a lot in the similar pairs and not at all in the worst 10.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- use larger corpus\n",
    "- more epochs to better memorize the dataset (if we only want to focus on WordSim353 and not generalize)\n",
    "- change hyperparameters (higher learning rate, ...)\n",
    "- refine model (add dropout, ...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiments = [\n",
    "    ('good', 'bad'),\n",
    "    ('ugly', 'beautiful'),\n",
    "    ('beautiful', 'amazing'),\n",
    "    ('bad', 'evil'),\n",
    "    \n",
    "    ('funny', 'weird'),\n",
    "    ('funny', 'good'),\n",
    "    \n",
    "    # these synonyms are not appearing in the corpus and are assigned to the <UNK> token\n",
    "    ('funny', 'curious'),\n",
    "    ('funny', 'amusiing'),\n",
    "    ('funny', 'hilarious'),\n",
    "    ('funny', 'entertaining')\n",
    "]\n",
    "\n",
    "for pair in sentiments:\n",
    "    word1, word2 = pair\n",
    "    word1_idx = cbow_dataloader.dataset.get_encoded_word(word1)\n",
    "    word2_idx = cbow_dataloader.dataset.get_encoded_word(word2)\n",
    "    word1_emb = cbow_model.embeddings.weight[word1_idx]\n",
    "    word2_emb = cbow_model.embeddings.weight[word2_idx]\n",
    "    cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "    \n",
    "    print(f\"{word1} ({word1_idx}), {word2} ({word2_idx}): {cosine_similarity}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cosine similarity for these words reflect the relation bewtween them. Opposites get lower scores than synonyms. Still the scores are not that far apart from each other, so it's maybe not a good basis to draw a conclusion.\n",
    "\n",
    "Words like 'funny' can be used for a good sentiment **and** sarcastically for a bad sentiment. We tried to compare synonyms for both sentiments, but found only 'bad' synonyms like 'weird' in the corpus, but no 'good' synonyms. Hence, funny has a rather high cosine similarity with 'bad' words and the 'good' meaning is not represented by the embedding. \n",
    "\n",
    "A bigger corpus would produce less unknown words and maybe generate better embeddings for the sentiment analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-corpus.50000.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs': 3,\n",
    "                      'batch_size': 8,\n",
    "                      'learning_rate': 0.001,\n",
    "                      'embedding_dim': 128,\n",
    "                      'output_dim': 128}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OCCURRENCE_THRESHOLD = 1\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, file, sentence_number):\n",
    "        samples = []\n",
    "        with open(file, encoding='UTF-8') as f:\n",
    "            lines = f.readlines()[:sentence_number]\n",
    "            occurrences = {}\n",
    "            for line in lines:\n",
    "                tokenized = [word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "                for word in tokenized:\n",
    "                    occurrences[word] = occurrences.get(word, 0) + 1\n",
    "\n",
    "            unknown_words = [word for word in occurrences if occurrences[word] <= OCCURRENCE_THRESHOLD]\n",
    "\n",
    "            for line in lines:\n",
    "                tokenized = [UNKNOWN_TOKEN if word in unknown_words else word.lower() for word in line.rstrip('\\n').split(' ')]\n",
    "                samples.append(tokenized)\n",
    "\n",
    "\n",
    "        vocab_list = [PADDING_TOKEN, *list(occurrences.keys()), UNKNOWN_TOKEN, START_TOKEN, END_TOKEN]\n",
    "        self.vocab = {word: index for index, word in enumerate(vocab_list)}\n",
    "        self.data = samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        encoded_sentence = [self.vocab[word] for word in sentence]\n",
    "        Sample = namedtuple('Sample', 'train eval')\n",
    "\n",
    "        train = [self.vocab[START_TOKEN], *encoded_sentence]\n",
    "        evaluation = [*encoded_sentence, self.vocab[END_TOKEN]]\n",
    "        return Sample(torch.tensor(train, device=device), torch.tensor(evaluation, device=device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_encoded_word(self, word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return self.vocab[UNKNOWN_TOKEN]\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dataset = LMDataset('data/wiki-corpus.50000.txt', 10000)\n",
    "#dataset[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lm_data(path, batch_size):\n",
    "    dataset = LMDataset(path, 10000)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: x)\n",
    "\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions, out_dim, num_layers):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings =  nn.Embedding(vocab_size, dimensions)\n",
    "        self.LSTM = nn.LSTM(dimensions, out_dim, num_layers=num_layers)\n",
    "        self.predict_word = nn.Linear(out_dim, vocab_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_reprentation, *_ = self.LSTM(embedded_seq)\n",
    "        predicted_words = self.predict_word(timestep_reprentation)\n",
    "        \n",
    "        del embedded_seq\n",
    "        del timestep_reprentation\n",
    "        return predicted_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "lm_dataloader = get_lm_data(data_path + 'wiki-corpus.50000.txt', lm_hyperparameters['batch_size'])\n",
    "\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(lm_dataloader.dataset.get_vocab_size(),\n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'],\n",
    "                       1)\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['learning_rate'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    print(f'batch count: {math.floor(len(lm_dataloader.dataset) / lm_dataloader.batch_size)}')\n",
    "    for i, batch in enumerate(lm_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "\n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = pad_sequence([sample.train for sample in batch])\n",
    "        gold_data = pad_sequence([sample.eval for sample in batch])\n",
    "        \n",
    "        input_sentence = input_sentence.to(device)\n",
    "        gold_data = gold_data.to(device)\n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "\n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "\n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        loss = loss_fn(output.view(-1, lm_dataloader.dataset.get_vocab_size()), gold_data.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(f'epoch {epoch},', f'batch {i}:', np.round(total_loss / (i + 1), 4), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        del input_sentence\n",
    "        del gold_data\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "\n",
    "def evaluate_model(path, dataloader, lm_model):\n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "\n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = good_s.split(' ')\n",
    "            tok_bad_s = bad_s.split(' ')\n",
    "\n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "               \n",
    "            \n",
    "            enc_good_s = torch.tensor([dataloader.dataset.get_encoded_word(x.lower()) for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([dataloader.dataset.get_encoded_word(x.lower()) for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "\n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = lm_model(enc_good_s)\n",
    "            bad_s = lm_model(enc_bad_s)\n",
    "\n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(good_s[0], dim=1)\n",
    "            bs_probs = F.softmax(bad_s[0], dim=1)\n",
    "            \n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s[0])\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s[0])\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob > bs_sent_prob))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def find_token_probs(model_probs, encoded_sentence):\n",
    "    probs = []\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for index, gold_token in enumerate(encoded_sentence):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = model_probs[index][gold_token]\n",
    "        probs.append(prob)\n",
    "    sentence_prob = sum(probs)\n",
    "    return sentence_prob\n",
    "\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(data_path + path, lm_dataloader, lm_model)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "#### full wiki-50000 (min-freq=1, number_sentences=10000)\n",
    "```\n",
    "batch count: 1250\n",
    "epoch 0, batch 1249: 3.8109\n",
    "batch count: 1250\n",
    "epoch 1, batch 1249: 3.3558\n",
    "batch count: 1250\n",
    "epoch 2, batch 1249: 3.1512\n",
    "CPU times: user 40.3 s, sys: 25.3 s, total: 1min 5s\n",
    "Wall time: 1min 6s\n",
    "\n",
    "Final accuracy:\n",
    "0.677\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('lm.pickle', 'wb') as f:\n",
    "    pickle.dump((lm_model, lm_dataloader), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('lm.pickle', 'rb') as f:\n",
    "    lm_model, lm_dataloader = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Having this small dataset with only 10000 sentences, our model still predicts 67 percent correctly. This seems quite good in our view. In any case, it also depends on the task, in which the model is used. Some critical tasks (e.g. in medical area) require a very high accuracy and this model wouldn't be enough. \n",
    "\n",
    "A random prediction approach would predict 50% correctly, so any model which performs better than this baseline has learned at least some embeddings correctly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "- larger dataset (lm_model uses sentences for training, so the actual dataset is even smaller than for the cbow_model)\n",
    "- more epochs (but since dataset is smaller and we want to generalize more, we shouldn't use too many epochs)\n",
    "- implement dropout (as for cbow_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "We could use recall and precision. This would give more insight for specific tasks (e.g. critical tasks/medical area)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "\n",
    "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "\n",
    "[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "\n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 63 marks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Having this small dataset with only 10000 sentences, our model still predicts 67 percent correctly. This seems quite good in our view. In any case, it also depends on the task, in which the model is used. Some critical tasks (e.g. in medical area) require a very high accuracy and this model wouldn't be enough. \n",
    "\n",
    "A random prediction approach would predict 50% correctly, so any model which performs better than this baseline has learned at least some embeddings correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "- larger dataset (lm_model uses sentences for training, so the actual dataset is even smaller than for the cbow_model)\n",
    "- more epochs (but since dataset is smaller and we want to generalize more, we shouldn't use too many epochs)\n",
    "- implement dropout (as for cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "We could use recall and precision. This would give more insight for specific tasks (e.g. critical tasks/medical area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "\n",
    "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "\n",
    "[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "\n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 63 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}