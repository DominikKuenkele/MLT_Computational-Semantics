{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we need to load some data, you can download the file on Canvas under files/assignments/03-lab-data/wiki-corpus.50000.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "data_path = 'data/'\n",
    "WINDOW_SIZE = 4\n",
    "\n",
    "\n",
    "def corpus_reader(data_path):\n",
    "    before = math.floor(WINDOW_SIZE / 2)\n",
    "    after = WINDOW_SIZE - before\n",
    "\n",
    "    samples = []\n",
    "    with open(data_path, encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            tokenized = [word.lower() for word in line.split(' ')]\n",
    "            for index, word in enumerate(tokenized):\n",
    "                context = []\n",
    "                for i in range(index - before, index):\n",
    "                    if i >= 0:\n",
    "                        context.append(tokenized[i])\n",
    "                for i in range(index + 1, index + after + 1):\n",
    "                    if i < len(tokenized):\n",
    "                        context.append(tokenized[i])\n",
    "                samples.append((word, context))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anarchist', ['historian', 'george']), ('historian', ['anarchist', 'george', 'woodcock']), ('george', ['anarchist', 'historian', 'woodcock', 'reports']), ('woodcock', ['historian', 'george', 'reports', 'that']), ('reports', ['george', 'woodcock', 'that', '\"']), ('that', ['woodcock', 'reports', '\"', 'the']), ('\"', ['reports', 'that', 'the', 'annual']), ('the', ['that', '\"', 'annual', 'congress']), ('annual', ['\"', 'the', 'congress', 'of']), ('congress', ['the', 'annual', 'of', 'the']), ('of', ['annual', 'congress', 'the', 'international']), ('the', ['congress', 'of', 'international', 'had']), ('international', ['of', 'the', 'had', 'not']), ('had', ['the', 'international', 'not', 'taken']), ('not', ['international', 'had', 'taken', 'place']), ('taken', ['had', 'not', 'place', 'in']), ('place', ['not', 'taken', 'in', '1870']), ('in', ['taken', 'place', '1870', 'owing']), ('1870', ['place', 'in', 'owing', 'to']), ('owing', ['in', '1870', 'to', 'the']), ('to', ['1870', 'owing', 'the', 'outbreak']), ('the', ['owing', 'to', 'outbreak', 'of']), ('outbreak', ['to', 'the', 'of', 'the']), ('of', ['the', 'outbreak', 'the', 'paris']), ('the', ['outbreak', 'of', 'paris', 'commune']), ('paris', ['of', 'the', 'commune', ',']), ('commune', ['the', 'paris', ',', 'and']), (',', ['paris', 'commune', 'and', 'in']), ('and', ['commune', ',', 'in', '1871']), ('in', [',', 'and', '1871', 'the']), ('1871', ['and', 'in', 'the', 'general']), ('the', ['in', '1871', 'general', 'council']), ('general', ['1871', 'the', 'council', 'called']), ('council', ['the', 'general', 'called', 'only']), ('called', ['general', 'council', 'only', 'a']), ('only', ['council', 'called', 'a', 'special']), ('a', ['called', 'only', 'special', 'conference']), ('special', ['only', 'a', 'conference', 'in']), ('conference', ['a', 'special', 'in', 'london']), ('in', ['special', 'conference', 'london', '.\\n']), ('london', ['conference', 'in', '.\\n']), ('.\\n', ['in', 'london']), ('a', ['bomb', 'was']), ('bomb', ['a', 'was', 'thrown']), ('was', ['a', 'bomb', 'thrown', 'by']), ('thrown', ['bomb', 'was', 'by', 'an']), ('by', ['was', 'thrown', 'an', 'unknown']), ('an', ['thrown', 'by', 'unknown', 'party']), ('unknown', ['by', 'an', 'party', 'near']), ('party', ['an', 'unknown', 'near', 'the']), ('near', ['unknown', 'party', 'the', 'conclusion']), ('the', ['party', 'near', 'conclusion', 'of']), ('conclusion', ['near', 'the', 'of', 'the']), ('of', ['the', 'conclusion', 'the', 'rally']), ('the', ['conclusion', 'of', 'rally', ',']), ('rally', ['of', 'the', ',', 'killing']), (',', ['the', 'rally', 'killing', 'an']), ('killing', ['rally', ',', 'an', 'officer']), ('an', [',', 'killing', 'officer', '.\\n']), ('officer', ['killing', 'an', '.\\n']), ('.\\n', ['an', 'officer']), ('in', ['the', 'ensuing']), ('the', ['in', 'ensuing', 'panic']), ('ensuing', ['in', 'the', 'panic', ',']), ('panic', ['the', 'ensuing', ',', 'police']), (',', ['ensuing', 'panic', 'police', 'opened']), ('police', ['panic', ',', 'opened', 'fire']), ('opened', [',', 'police', 'fire', 'on']), ('fire', ['police', 'opened', 'on', 'the']), ('on', ['opened', 'fire', 'the', 'crowd']), ('the', ['fire', 'on', 'crowd', 'and']), ('crowd', ['on', 'the', 'and', 'each']), ('and', ['the', 'crowd', 'each', 'other']), ('each', ['crowd', 'and', 'other', '.\\n']), ('other', ['and', 'each', '.\\n']), ('.\\n', ['each', 'other']), ('josiah', ['warren', 'is']), ('warren', ['josiah', 'is', 'widely']), ('is', ['josiah', 'warren', 'widely', 'regarded']), ('widely', ['warren', 'is', 'regarded', 'as']), ('regarded', ['is', 'widely', 'as', 'the']), ('as', ['widely', 'regarded', 'the', 'first']), ('the', ['regarded', 'as', 'first', 'american']), ('first', ['as', 'the', 'american', 'anarchist']), ('american', ['the', 'first', 'anarchist', ',']), ('anarchist', ['first', 'american', ',', 'and']), (',', ['american', 'anarchist', 'and', 'the']), ('and', ['anarchist', ',', 'the', 'four-page']), ('the', [',', 'and', 'four-page', 'weekly']), ('four-page', ['and', 'the', 'weekly', 'paper']), ('weekly', ['the', 'four-page', 'paper', 'he']), ('paper', ['four-page', 'weekly', 'he', 'edited']), ('he', ['weekly', 'paper', 'edited', 'during']), ('edited', ['paper', 'he', 'during', '1833']), ('during', ['he', 'edited', '1833', ',']), ('1833', ['edited', 'during', ',', 'the']), (',', ['during', '1833', 'the', 'peaceful']), ('the', ['1833', ',', 'peaceful', 'revolutionist']), ('peaceful', [',', 'the', 'revolutionist', ',']), ('revolutionist', ['the', 'peaceful', ',', 'was']), (',', ['peaceful', 'revolutionist', 'was', 'the']), ('was', ['revolutionist', ',', 'the', 'first']), ('the', [',', 'was', 'first', 'anarchist']), ('first', ['was', 'the', 'anarchist', 'periodical']), ('anarchist', ['the', 'first', 'periodical', 'published']), ('periodical', ['first', 'anarchist', 'published', '.\\n']), ('published', ['anarchist', 'periodical', '.\\n']), ('.\\n', ['periodical', 'published']), ('weak', ['central', 'coherence']), ('central', ['weak', 'coherence', 'theory']), ('coherence', ['weak', 'central', 'theory', 'hypothesizes']), ('theory', ['central', 'coherence', 'hypothesizes', 'that']), ('hypothesizes', ['coherence', 'theory', 'that', 'a']), ('that', ['theory', 'hypothesizes', 'a', 'limited']), ('a', ['hypothesizes', 'that', 'limited', 'ability']), ('limited', ['that', 'a', 'ability', 'to']), ('ability', ['a', 'limited', 'to', 'see']), ('to', ['limited', 'ability', 'see', 'the']), ('see', ['ability', 'to', 'the', 'big']), ('the', ['to', 'see', 'big', 'picture']), ('big', ['see', 'the', 'picture', 'underlies']), ('picture', ['the', 'big', 'underlies', 'the']), ('underlies', ['big', 'picture', 'the', 'central']), ('the', ['picture', 'underlies', 'central', 'disturbance']), ('central', ['underlies', 'the', 'disturbance', 'in']), ('disturbance', ['the', 'central', 'in', 'autism']), ('in', ['central', 'disturbance', 'autism', '.\\n']), ('autism', ['disturbance', 'in', '.\\n']), ('.\\n', ['in', 'autism']), ('the', ['word', 'autism']), ('word', ['the', 'autism', 'first']), ('autism', ['the', 'word', 'first', 'took']), ('first', ['word', 'autism', 'took', 'its']), ('took', ['autism', 'first', 'its', 'modern']), ('its', ['first', 'took', 'modern', 'sense']), ('modern', ['took', 'its', 'sense', 'in']), ('sense', ['its', 'modern', 'in', '1938']), ('in', ['modern', 'sense', '1938', 'when']), ('1938', ['sense', 'in', 'when', 'hans']), ('when', ['in', '1938', 'hans', 'asperger']), ('hans', ['1938', 'when', 'asperger', 'of']), ('asperger', ['when', 'hans', 'of', 'the']), ('of', ['hans', 'asperger', 'the', 'vienna']), ('the', ['asperger', 'of', 'vienna', 'university']), ('vienna', ['of', 'the', 'university', 'hospital']), ('university', ['the', 'vienna', 'hospital', 'adopted']), ('hospital', ['vienna', 'university', 'adopted', 'bleuler']), ('adopted', ['university', 'hospital', 'bleuler', \"'s\"]), ('bleuler', ['hospital', 'adopted', \"'s\", 'terminology']), (\"'s\", ['adopted', 'bleuler', 'terminology', 'autistic']), ('terminology', ['bleuler', \"'s\", 'autistic', 'psychopaths']), ('autistic', [\"'s\", 'terminology', 'psychopaths', 'in']), ('psychopaths', ['terminology', 'autistic', 'in', 'a']), ('in', ['autistic', 'psychopaths', 'a', 'lecture']), ('a', ['psychopaths', 'in', 'lecture', 'in']), ('lecture', ['in', 'a', 'in', 'german']), ('in', ['a', 'lecture', 'german', 'about']), ('german', ['lecture', 'in', 'about', 'child']), ('about', ['in', 'german', 'child', 'psychology']), ('child', ['german', 'about', 'psychology', '.\\n']), ('psychology', ['about', 'child', '.\\n']), ('.\\n', ['child', 'psychology']), ('most', ['land', 'areas']), ('land', ['most', 'areas', 'are']), ('areas', ['most', 'land', 'are', 'in']), ('are', ['land', 'areas', 'in', 'an']), ('in', ['areas', 'are', 'an', 'albedo']), ('an', ['are', 'in', 'albedo', 'range']), ('albedo', ['in', 'an', 'range', 'of']), ('range', ['an', 'albedo', 'of', '0.1']), ('of', ['albedo', 'range', '0.1', 'to']), ('0.1', ['range', 'of', 'to', '0.4']), ('to', ['of', '0.1', '0.4', '.\\n']), ('0.4', ['0.1', 'to', '.\\n']), ('.\\n', ['to', '0.4']), ('now', ['expanded', 'to']), ('expanded', ['now', 'to', 'nine']), ('to', ['now', 'expanded', 'nine', ',']), ('nine', ['expanded', 'to', ',', 'these']), (',', ['to', 'nine', 'these', 'include']), ('these', ['nine', ',', 'include', 'the']), ('include', [',', 'these', 'the', 'poarch']), ('the', ['these', 'include', 'poarch', 'band']), ('poarch', ['include', 'the', 'band', 'of']), ('band', ['the', 'poarch', 'of', 'creek']), ('of', ['poarch', 'band', 'creek', 'indians']), ('creek', ['band', 'of', 'indians', ',']), ('indians', ['of', 'creek', ',', 'mowa']), (',', ['creek', 'indians', 'mowa', 'band']), ('mowa', ['indians', ',', 'band', 'of']), ('band', [',', 'mowa', 'of', 'choctaw']), ('of', ['mowa', 'band', 'choctaw', 'indians']), ('choctaw', ['band', 'of', 'indians', ',']), ('indians', ['of', 'choctaw', ',', 'star']), (',', ['choctaw', 'indians', 'star', 'clan']), ('star', ['indians', ',', 'clan', 'of']), ('clan', [',', 'star', 'of', 'muscogee']), ('of', ['star', 'clan', 'muscogee', 'creeks']), ('muscogee', ['clan', 'of', 'creeks', ',']), ('creeks', ['of', 'muscogee', ',', 'echota']), (',', ['muscogee', 'creeks', 'echota', 'cherokee']), ('echota', ['creeks', ',', 'cherokee', 'tribe']), ('cherokee', [',', 'echota', 'tribe', 'of']), ('tribe', ['echota', 'cherokee', 'of', 'alabama']), ('of', ['cherokee', 'tribe', 'alabama', ',']), ('alabama', ['tribe', 'of', ',', 'cherokees']), (',', ['of', 'alabama', 'cherokees', 'of']), ('cherokees', ['alabama', ',', 'of', 'northeast']), ('of', [',', 'cherokees', 'northeast', 'alabama']), ('northeast', ['cherokees', 'of', 'alabama', ',']), ('alabama', ['of', 'northeast', ',', 'cherokees']), (',', ['northeast', 'alabama', 'cherokees', 'of']), ('cherokees', ['alabama', ',', 'of', 'southeast']), ('of', [',', 'cherokees', 'southeast', 'alabama']), ('southeast', ['cherokees', 'of', 'alabama', ',']), ('alabama', ['of', 'southeast', ',', 'ma-chis']), (',', ['southeast', 'alabama', 'ma-chis', 'lower']), ('ma-chis', ['alabama', ',', 'lower', 'creek']), ('lower', [',', 'ma-chis', 'creek', 'indian']), ('creek', ['ma-chis', 'lower', 'indian', 'tribe']), ('indian', ['lower', 'creek', 'tribe', ',']), ('tribe', ['creek', 'indian', ',', 'piqua']), (',', ['indian', 'tribe', 'piqua', 'sept']), ('piqua', ['tribe', ',', 'sept', 'of']), ('sept', [',', 'piqua', 'of', 'ohio']), ('of', ['piqua', 'sept', 'ohio', 'shawnee']), ('ohio', ['sept', 'of', 'shawnee', 'tribe']), ('shawnee', ['of', 'ohio', 'tribe', ',']), ('tribe', ['ohio', 'shawnee', ',', 'and']), (',', ['shawnee', 'tribe', 'and', 'united']), ('and', ['tribe', ',', 'united', 'cherokee']), ('united', [',', 'and', 'cherokee', 'ani-yun-wiya']), ('cherokee', ['and', 'united', 'ani-yun-wiya', 'nation']), ('ani-yun-wiya', ['united', 'cherokee', 'nation', '.\\n']), ('nation', ['cherokee', 'ani-yun-wiya', '.\\n']), ('.\\n', ['ani-yun-wiya', 'nation']), ('of', ['those', 'who']), ('those', ['of', 'who', 'indicated']), ('who', ['of', 'those', 'indicated', 'a']), ('indicated', ['those', 'who', 'a', 'religious']), ('a', ['who', 'indicated', 'religious', 'preference']), ('religious', ['indicated', 'a', 'preference', ',']), ('preference', ['a', 'religious', ',', '59']), (',', ['religious', 'preference', '59', '%']), ('59', ['preference', ',', '%', 'said']), ('%', [',', '59', 'said', 'they']), ('said', ['59', '%', 'they', 'possessed']), ('they', ['%', 'said', 'possessed', 'a']), ('possessed', ['said', 'they', 'a', '\"']), ('a', ['they', 'possessed', '\"', 'full']), ('\"', ['possessed', 'a', 'full', 'understanding']), ('full', ['a', '\"', 'understanding', '\"']), ('understanding', ['\"', 'full', '\"', 'of']), ('\"', ['full', 'understanding', 'of', 'their']), ('of', ['understanding', '\"', 'their', 'faith']), ('their', ['\"', 'of', 'faith', 'and']), ('faith', ['of', 'their', 'and', 'needed']), ('and', ['their', 'faith', 'needed', 'no']), ('needed', ['faith', 'and', 'no', 'further']), ('no', ['and', 'needed', 'further', 'learning']), ('further', ['needed', 'no', 'learning', '.\\n']), ('learning', ['no', 'further', '.\\n']), ('.\\n', ['further', 'learning']), ('once', ['he', 'realized']), ('he', ['once', 'realized', 'that']), ('realized', ['once', 'he', 'that', 'his']), ('that', ['he', 'realized', 'his', 'distraction']), ('his', ['realized', 'that', 'distraction', 'was']), ('distraction', ['that', 'his', 'was', 'endangering']), ('was', ['his', 'distraction', 'endangering', 'his']), ('endangering', ['distraction', 'was', 'his', 'life']), ('his', ['was', 'endangering', 'life', ',']), ('life', ['endangering', 'his', ',', 'he']), (',', ['his', 'life', 'he', 'refocused']), ('he', ['life', ',', 'refocused', 'and']), ('refocused', [',', 'he', 'and', 'killed']), ('and', ['he', 'refocused', 'killed', 'her']), ('killed', ['refocused', 'and', 'her', '.\\n']), ('her', ['and', 'killed', '.\\n']), ('.\\n', ['killed', 'her']), ('pausanias', ['has', 'been']), ('has', ['pausanias', 'been', 'told']), ('been', ['pausanias', 'has', 'told', 'that']), ('told', ['has', 'been', 'that', 'the']), ('that', ['been', 'told', 'the', 'island']), ('the', ['told', 'that', 'island', 'is']), ('island', ['that', 'the', 'is', '\"']), ('is', ['the', 'island', '\"', 'covered']), ('\"', ['island', 'is', 'covered', 'with']), ('covered', ['is', '\"', 'with', 'forests']), ('with', ['\"', 'covered', 'forests', 'and']), ('forests', ['covered', 'with', 'and', 'full']), ('and', ['with', 'forests', 'full', 'of']), ('full', ['forests', 'and', 'of', 'animals']), ('of', ['and', 'full', 'animals', ',']), ('animals', ['full', 'of', ',', 'some']), (',', ['of', 'animals', 'some', 'wild']), ('some', ['animals', ',', 'wild', ',']), ('wild', [',', 'some', ',', 'some']), (',', ['some', 'wild', 'some', 'tame']), ('some', ['wild', ',', 'tame', '.\\n']), ('tame', [',', 'some', '.\\n']), ('.\\n', ['some', 'tame']), ('in', ['indiana', ',']), ('indiana', ['in', ',', 'when']), (',', ['in', 'indiana', 'when', 'lincoln']), ('when', ['indiana', ',', 'lincoln', 'was']), ('lincoln', [',', 'when', 'was', 'nine']), ('was', ['when', 'lincoln', 'nine', ',']), ('nine', ['lincoln', 'was', ',', 'his']), (',', ['was', 'nine', 'his', 'mother']), ('his', ['nine', ',', 'mother', 'nancy']), ('mother', [',', 'his', 'nancy', 'died']), ('nancy', ['his', 'mother', 'died', 'of']), ('died', ['mother', 'nancy', 'of', 'milk']), ('of', ['nancy', 'died', 'milk', 'sickness']), ('milk', ['died', 'of', 'sickness', 'in']), ('sickness', ['of', 'milk', 'in', '1818']), ('in', ['milk', 'sickness', '1818', '.\\n']), ('1818', ['sickness', 'in', '.\\n']), ('.\\n', ['in', '1818']), ('canoeing', ['down', 'the']), ('down', ['canoeing', 'the', 'sangamon']), ('the', ['canoeing', 'down', 'sangamon', 'river']), ('sangamon', ['down', 'the', 'river', ',']), ('river', ['the', 'sangamon', ',', 'lincoln']), (',', ['sangamon', 'river', 'lincoln', 'ended']), ('lincoln', ['river', ',', 'ended', 'up']), ('ended', [',', 'lincoln', 'up', 'in']), ('up', ['lincoln', 'ended', 'in', 'the']), ('in', ['ended', 'up', 'the', 'village']), ('the', ['up', 'in', 'village', 'of']), ('village', ['in', 'the', 'of', 'new']), ('of', ['the', 'village', 'new', 'salem']), ('new', ['village', 'of', 'salem', 'in']), ('salem', ['of', 'new', 'in', 'sangamon']), ('in', ['new', 'salem', 'sangamon', 'county']), ('sangamon', ['salem', 'in', 'county', '.\\n']), ('county', ['in', 'sangamon', '.\\n']), ('.\\n', ['sangamon', 'county']), ('the', ['lincolns', \"'\"]), ('lincolns', ['the', \"'\", 'fourth']), (\"'\", ['the', 'lincolns', 'fourth', 'son']), ('fourth', ['lincolns', \"'\", 'son', ',']), ('son', [\"'\", 'fourth', ',', 'thomas']), (',', ['fourth', 'son', 'thomas', '\"']), ('thomas', ['son', ',', '\"', 'tad']), ('\"', [',', 'thomas', 'tad', '\"']), ('tad', ['thomas', '\"', '\"', 'lincoln']), ('\"', ['\"', 'tad', 'lincoln', ',']), ('lincoln', ['tad', '\"', ',', 'was']), (',', ['\"', 'lincoln', 'was', 'born']), ('was', ['lincoln', ',', 'born', 'on']), ('born', [',', 'was', 'on', 'april']), ('on', ['was', 'born', 'april', '4']), ('april', ['born', 'on', '4', ',']), ('4', ['on', 'april', ',', '1853']), (',', ['april', '4', '1853', ',']), ('1853', ['4', ',', ',', 'and']), (',', [',', '1853', 'and', 'died']), ('and', ['1853', ',', 'died', 'of']), ('died', [',', 'and', 'of', 'heart']), ('of', ['and', 'died', 'heart', 'failure']), ('heart', ['died', 'of', 'failure', 'at']), ('failure', ['of', 'heart', 'at', 'the']), ('at', ['heart', 'failure', 'the', 'age']), ('the', ['failure', 'at', 'age', 'of']), ('age', ['at', 'the', 'of', '18']), ('of', ['the', 'age', '18', 'on']), ('18', ['age', 'of', 'on', 'july']), ('on', ['of', '18', 'july', '16']), ('july', ['18', 'on', '16', ',']), ('16', ['on', 'july', ',', '1871']), (',', ['july', '16', '1871', '.\\n']), ('1871', ['16', ',', '.\\n']), ('.\\n', [',', '1871']), ('lincoln', ['also', 'supported']), ('also', ['lincoln', 'supported', 'the']), ('supported', ['lincoln', 'also', 'the', 'wilmot']), ('the', ['also', 'supported', 'wilmot', 'proviso']), ('wilmot', ['supported', 'the', 'proviso', ',']), ('proviso', ['the', 'wilmot', ',', 'which']), (',', ['wilmot', 'proviso', 'which', ',']), ('which', ['proviso', ',', ',', 'if']), (',', [',', 'which', 'if', 'it']), ('if', ['which', ',', 'it', 'had']), ('it', [',', 'if', 'had', 'been']), ('had', ['if', 'it', 'been', 'adopted']), ('been', ['it', 'had', 'adopted', ',']), ('adopted', ['had', 'been', ',', 'would']), (',', ['been', 'adopted', 'would', 'have']), ('would', ['adopted', ',', 'have', 'banned']), ('have', [',', 'would', 'banned', 'slavery']), ('banned', ['would', 'have', 'slavery', 'in']), ('slavery', ['have', 'banned', 'in', 'any']), ('in', ['banned', 'slavery', 'any', 'u.s.']), ('any', ['slavery', 'in', 'u.s.', 'territory']), ('u.s.', ['in', 'any', 'territory', 'won']), ('territory', ['any', 'u.s.', 'won', 'from']), ('won', ['u.s.', 'territory', 'from', 'mexico']), ('from', ['territory', 'won', 'mexico', '.\\n']), ('mexico', ['won', 'from', '.\\n']), ('.\\n', ['from', 'mexico']), ('lincoln', ['emphasized', 'his']), ('emphasized', ['lincoln', 'his', 'opposition']), ('his', ['lincoln', 'emphasized', 'opposition', 'to']), ('opposition', ['emphasized', 'his', 'to', 'polk']), ('to', ['his', 'opposition', 'polk', 'by']), ('polk', ['opposition', 'to', 'by', 'drafting']), ('by', ['to', 'polk', 'drafting', 'and']), ('drafting', ['polk', 'by', 'and', 'introducing']), ('and', ['by', 'drafting', 'introducing', 'his']), ('introducing', ['drafting', 'and', 'his', 'spot']), ('his', ['and', 'introducing', 'spot', 'resolutions']), ('spot', ['introducing', 'his', 'resolutions', '.\\n']), ('resolutions', ['his', 'spot', '.\\n']), ('.\\n', ['spot', 'resolutions']), ('realizing', ['clay', 'was']), ('clay', ['realizing', 'was', 'unlikely']), ('was', ['realizing', 'clay', 'unlikely', 'to']), ('unlikely', ['clay', 'was', 'to', 'win']), ('to', ['was', 'unlikely', 'win', 'the']), ('win', ['unlikely', 'to', 'the', 'presidency']), ('the', ['to', 'win', 'presidency', ',']), ('presidency', ['win', 'the', ',', 'lincoln']), (',', ['the', 'presidency', 'lincoln', ',']), ('lincoln', ['presidency', ',', ',', 'who']), (',', [',', 'lincoln', 'who', 'had']), ('who', ['lincoln', ',', 'had', 'pledged']), ('had', [',', 'who', 'pledged', 'in']), ('pledged', ['who', 'had', 'in', '1846']), ('in', ['had', 'pledged', '1846', 'to']), ('1846', ['pledged', 'in', 'to', 'serve']), ('to', ['in', '1846', 'serve', 'only']), ('serve', ['1846', 'to', 'only', 'one']), ('only', ['to', 'serve', 'one', 'term']), ('one', ['serve', 'only', 'term', 'in']), ('term', ['only', 'one', 'in', 'the']), ('in', ['one', 'term', 'the', 'house']), ('the', ['term', 'in', 'house', ',']), ('house', ['in', 'the', ',', 'supported']), (',', ['the', 'house', 'supported', 'general']), ('supported', ['house', ',', 'general', 'zachary']), ('general', [',', 'supported', 'zachary', 'taylor']), ('zachary', ['supported', 'general', 'taylor', 'for']), ('taylor', ['general', 'zachary', 'for', 'the']), ('for', ['zachary', 'taylor', 'the', 'whig']), ('the', ['taylor', 'for', 'whig', 'nomination']), ('whig', ['for', 'the', 'nomination', 'in']), ('nomination', ['the', 'whig', 'in', 'the']), ('in', ['whig', 'nomination', 'the', '1848']), ('the', ['nomination', 'in', '1848', 'presidential']), ('1848', ['in', 'the', 'presidential', 'election']), ('presidential', ['the', '1848', 'election', '.\\n']), ('election', ['1848', 'presidential', '.\\n']), ('.\\n', ['presidential', 'election']), ('the', ['awards', 'were']), ('awards', ['the', 'were', 'first']), ('were', ['the', 'awards', 'first', 'given']), ('first', ['awards', 'were', 'given', 'in']), ('given', ['were', 'first', 'in', '1929']), ('in', ['first', 'given', '1929', 'at']), ('1929', ['given', 'in', 'at', 'a']), ('at', ['in', '1929', 'a', 'ceremony']), ('a', ['1929', 'at', 'ceremony', 'created']), ('ceremony', ['at', 'a', 'created', 'for']), ('created', ['a', 'ceremony', 'for', 'the']), ('for', ['ceremony', 'created', 'the', 'awards']), ('the', ['created', 'for', 'awards', ',']), ('awards', ['for', 'the', ',', 'at']), (',', ['the', 'awards', 'at', 'the']), ('at', ['awards', ',', 'the', 'hotel']), ('the', [',', 'at', 'hotel', 'roosevelt']), ('hotel', ['at', 'the', 'roosevelt', 'in']), ('roosevelt', ['the', 'hotel', 'in', 'hollywood']), ('in', ['hotel', 'roosevelt', 'hollywood', '.\\n']), ('hollywood', ['roosevelt', 'in', '.\\n']), ('.\\n', ['in', 'hollywood']), ('after', ['the', 'success']), ('the', ['after', 'success', 'of']), ('success', ['after', 'the', 'of', 'her']), ('of', ['the', 'success', 'her', 'later']), ('her', ['success', 'of', 'later', 'novels']), ('later', ['of', 'her', 'novels', ',']), ('novels', ['her', 'later', ',', 'rand']), (',', ['later', 'novels', 'rand', 'was']), ('rand', ['novels', ',', 'was', 'able']), ('was', [',', 'rand', 'able', 'to']), ('able', ['rand', 'was', 'to', 'release']), ('to', ['was', 'able', 'release', 'a']), ('release', ['able', 'to', 'a', 'revised']), ('a', ['to', 'release', 'revised', 'version']), ('revised', ['release', 'a', 'version', 'in']), ('version', ['a', 'revised', 'in', '1959']), ('in', ['revised', 'version', '1959', 'that']), ('1959', ['version', 'in', 'that', 'has']), ('that', ['in', '1959', 'has', 'since']), ('has', ['1959', 'that', 'since', 'sold']), ('since', ['that', 'has', 'sold', 'over']), ('sold', ['has', 'since', 'over', 'three']), ('over', ['since', 'sold', 'three', 'million']), ('three', ['sold', 'over', 'million', 'copies']), ('million', ['over', 'three', 'copies', '.\\n']), ('copies', ['three', 'million', '.\\n']), ('.\\n', ['million', 'copies']), ('career', ['dwan', 'operated']), ('dwan', ['career', 'operated', 'flying']), ('operated', ['career', 'dwan', 'flying', 'a']), ('flying', ['dwan', 'operated', 'a', 'studios']), ('a', ['operated', 'flying', 'studios', 'in']), ('studios', ['flying', 'a', 'in', 'la']), ('in', ['a', 'studios', 'la', 'mesa']), ('la', ['studios', 'in', 'mesa', ',']), ('mesa', ['in', 'la', ',', 'california']), (',', ['la', 'mesa', 'california', 'from']), ('california', ['mesa', ',', 'from', 'august']), ('from', [',', 'california', 'august', ',']), ('august', ['california', 'from', ',', '1911']), (',', ['from', 'august', '1911', 'to']), ('1911', ['august', ',', 'to', 'july']), ('to', [',', '1911', 'july', ',']), ('july', ['1911', 'to', ',', '1912']), (',', ['to', 'july', '1912', '.\\n']), ('1912', ['july', ',', '.\\n']), ('.\\n', [',', '1912']), ('ibn', ['khaldun', 'provides']), ('khaldun', ['ibn', 'provides', 'a']), ('provides', ['ibn', 'khaldun', 'a', 'table']), ('a', ['khaldun', 'provides', 'table', 'summarizing']), ('table', ['provides', 'a', 'summarizing', 'the']), ('summarizing', ['a', 'table', 'the', 'zirid']), ('the', ['table', 'summarizing', 'zirid', ',']), ('zirid', ['summarizing', 'the', ',', 'banu']), (',', ['the', 'zirid', 'banu', 'ifran']), ('banu', ['zirid', ',', 'ifran', ',']), ('ifran', [',', 'banu', ',', 'maghrawa']), (',', ['banu', 'ifran', 'maghrawa', ',']), ('maghrawa', ['ifran', ',', ',', 'almoravid']), (',', [',', 'maghrawa', 'almoravid', ',']), ('almoravid', ['maghrawa', ',', ',', 'hammadid']), (',', [',', 'almoravid', 'hammadid', ',']), ('hammadid', ['almoravid', ',', ',', 'almohad']), (',', [',', 'hammadid', 'almohad', ',']), ('almohad', ['hammadid', ',', ',', 'merinid']), (',', [',', 'almohad', 'merinid', ',']), ('merinid', ['almohad', ',', ',', 'abdalwadid']), (',', [',', 'merinid', 'abdalwadid', ',']), ('abdalwadid', ['merinid', ',', ',', 'wattasid']), (',', [',', 'abdalwadid', 'wattasid', ',']), ('wattasid', ['abdalwadid', ',', ',', 'meknassa']), (',', [',', 'wattasid', 'meknassa', 'and']), ('meknassa', ['wattasid', ',', 'and', 'hafsid']), ('and', [',', 'meknassa', 'hafsid', 'dynasties']), ('hafsid', ['meknassa', 'and', 'dynasties', '.\\n']), ('dynasties', ['and', 'hafsid', '.\\n']), ('.\\n', ['hafsid', 'dynasties']), ('the', ['forests', 'are']), ('forests', ['the', 'are', 'inhabited']), ('are', ['the', 'forests', 'inhabited', 'by']), ('inhabited', ['forests', 'are', 'by', 'boars']), ('by', ['are', 'inhabited', 'boars', 'and']), ('boars', ['inhabited', 'by', 'and', 'jackals']), ('and', ['by', 'boars', 'jackals', '.\\n']), ('jackals', ['boars', 'and', '.\\n']), ('.\\n', ['and', 'jackals']), ('russia', ['is', 'also']), ('is', ['russia', 'also', 'building']), ('also', ['russia', 'is', 'building', 'two']), ('building', ['is', 'also', 'two', '636-type']), ('two', ['also', 'building', '636-type', 'diesel']), ('636-type', ['building', 'two', 'diesel', 'submarines']), ('diesel', ['two', '636-type', 'submarines', 'for']), ('submarines', ['636-type', 'diesel', 'for', 'algeria']), ('for', ['diesel', 'submarines', 'algeria', '.\\n']), ('algeria', ['submarines', 'for', '.\\n']), ('.\\n', ['for', 'algeria']), ('the', ['undisputed', 'master']), ('undisputed', ['the', 'master', 'of']), ('master', ['the', 'undisputed', 'of', 'this']), ('of', ['undisputed', 'master', 'this', 'music']), ('this', ['master', 'of', 'music', 'is']), ('music', ['of', 'this', 'is', 'el']), ('is', ['this', 'music', 'el', 'hadj']), ('el', ['music', 'is', 'hadj', \"m'hamed\"]), ('hadj', ['is', 'el', \"m'hamed\", 'el']), (\"m'hamed\", ['el', 'hadj', 'el', 'anka']), ('el', ['hadj', \"m'hamed\", 'anka', '.\\n']), ('anka', [\"m'hamed\", 'el', '.\\n']), ('.\\n', ['el', 'anka']), ('best', ['universities', 'of']), ('universities', ['best', 'of', 'qualifications']), ('of', ['best', 'universities', 'qualifications', 'remain']), ('qualifications', ['universities', 'of', 'remain', 'the']), ('remain', ['of', 'qualifications', 'the', 'university']), ('the', ['qualifications', 'remain', 'university', 'of']), ('university', ['remain', 'the', 'of', 'tlemcen']), ('of', ['the', 'university', 'tlemcen', 'and']), ('tlemcen', ['university', 'of', 'and', 'batna']), ('and', ['of', 'tlemcen', 'batna', 'hadj']), ('batna', ['tlemcen', 'and', 'hadj', 'bereket']), ('hadj', ['and', 'batna', 'bereket', ',']), ('bereket', ['batna', 'hadj', ',', 'they']), (',', ['hadj', 'bereket', 'they', 'occupy']), ('they', ['bereket', ',', 'occupy', 'the']), ('occupy', [',', 'they', 'the', '26th']), ('the', ['they', 'occupy', '26th', 'and']), ('26th', ['occupy', 'the', 'and', '45th']), ('and', ['the', '26th', '45th', 'row']), ('45th', ['26th', 'and', 'row', 'in']), ('row', ['and', '45th', 'in', 'africa']), ('in', ['45th', 'row', 'africa', ',']), ('africa', ['row', 'in', ',', 'which']), (',', ['in', 'africa', 'which', 'is']), ('which', ['africa', ',', 'is', 'a']), ('is', [',', 'which', 'a', 'very']), ('a', ['which', 'is', 'very', 'bad']), ('very', ['is', 'a', 'bad', 'standing']), ('bad', ['a', 'very', 'standing', '.\\n']), ('standing', ['very', 'bad', '.\\n']), ('.\\n', ['bad', 'standing']), ('anthropologists', ['study', 'topics']), ('study', ['anthropologists', 'topics', 'including']), ('topics', ['anthropologists', 'study', 'including', 'the']), ('including', ['study', 'topics', 'the', 'origin']), ('the', ['topics', 'including', 'origin', 'and']), ('origin', ['including', 'the', 'and', 'evolution']), ('and', ['the', 'origin', 'evolution', 'of']), ('evolution', ['origin', 'and', 'of', 'homo']), ('of', ['and', 'evolution', 'homo', 'sapiens']), ('homo', ['evolution', 'of', 'sapiens', ',']), ('sapiens', ['of', 'homo', ',', 'the']), (',', ['homo', 'sapiens', 'the', 'organization']), ('the', ['sapiens', ',', 'organization', 'of']), ('organization', [',', 'the', 'of', 'human']), ('of', ['the', 'organization', 'human', 'social']), ('human', ['organization', 'of', 'social', 'and']), ('social', ['of', 'human', 'and', 'cultural']), ('and', ['human', 'social', 'cultural', 'relations']), ('cultural', ['social', 'and', 'relations', ',']), ('relations', ['and', 'cultural', ',', 'human']), (',', ['cultural', 'relations', 'human', 'physical']), ('human', ['relations', ',', 'physical', 'traits']), ('physical', [',', 'human', 'traits', ',']), ('traits', ['human', 'physical', ',', 'human']), (',', ['physical', 'traits', 'human', 'behavior']), ('human', ['traits', ',', 'behavior', ',']), ('behavior', [',', 'human', ',', 'the']), (',', ['human', 'behavior', 'the', 'variations']), ('the', ['behavior', ',', 'variations', 'among']), ('variations', [',', 'the', 'among', 'different']), ('among', ['the', 'variations', 'different', 'groups']), ('different', ['variations', 'among', 'groups', 'of']), ('groups', ['among', 'different', 'of', 'humans']), ('of', ['different', 'groups', 'humans', ',']), ('humans', ['groups', 'of', ',', 'how']), (',', ['of', 'humans', 'how', 'the']), ('how', ['humans', ',', 'the', 'evolutionary']), ('the', [',', 'how', 'evolutionary', 'past']), ('evolutionary', ['how', 'the', 'past', 'of']), ('past', ['the', 'evolutionary', 'of', 'homo']), ('of', ['evolutionary', 'past', 'homo', 'sapiens']), ('homo', ['past', 'of', 'sapiens', 'has']), ('sapiens', ['of', 'homo', 'has', 'influenced']), ('has', ['homo', 'sapiens', 'influenced', 'its']), ('influenced', ['sapiens', 'has', 'its', 'social']), ('its', ['has', 'influenced', 'social', 'organization']), ('social', ['influenced', 'its', 'organization', 'and']), ('organization', ['its', 'social', 'and', 'culture']), ('and', ['social', 'organization', 'culture', ',']), ('culture', ['organization', 'and', ',', 'and']), (',', ['and', 'culture', 'and', 'so']), ('and', ['culture', ',', 'so', 'forth']), ('so', [',', 'and', 'forth', '.\\n']), ('forth', ['and', 'so', '.\\n']), ('.\\n', ['so', 'forth']), ('the', ['alchemist', 'robert']), ('alchemist', ['the', 'robert', 'boyle']), ('robert', ['the', 'alchemist', 'boyle', 'is']), ('boyle', ['alchemist', 'robert', 'is', 'credited']), ('is', ['robert', 'boyle', 'credited', 'as']), ('credited', ['boyle', 'is', 'as', 'being']), ('as', ['is', 'credited', 'being', 'the']), ('being', ['credited', 'as', 'the', 'father']), ('the', ['as', 'being', 'father', 'of']), ('father', ['being', 'the', 'of', 'chemistry']), ('of', ['the', 'father', 'chemistry', '.\\n']), ('chemistry', ['father', 'of', '.\\n']), ('.\\n', ['of', 'chemistry']), ('alchemy', ['coexisted', 'alongside']), ('coexisted', ['alchemy', 'alongside', 'emerging']), ('alongside', ['alchemy', 'coexisted', 'emerging', 'christianity']), ('emerging', ['coexisted', 'alongside', 'christianity', '.\\n']), ('christianity', ['alongside', 'emerging', '.\\n']), ('.\\n', ['emerging', 'christianity']), ('his', ['writing', 'portrayed']), ('writing', ['his', 'portrayed', 'alchemy']), ('portrayed', ['his', 'writing', 'alchemy', 'as']), ('alchemy', ['writing', 'portrayed', 'as', 'a']), ('as', ['portrayed', 'alchemy', 'a', 'sort']), ('a', ['alchemy', 'as', 'sort', 'of']), ('sort', ['as', 'a', 'of', 'terrestrial']), ('of', ['a', 'sort', 'terrestrial', 'astronomy']), ('terrestrial', ['sort', 'of', 'astronomy', 'in']), ('astronomy', ['of', 'terrestrial', 'in', 'line']), ('in', ['terrestrial', 'astronomy', 'line', 'with']), ('line', ['astronomy', 'in', 'with', 'the']), ('with', ['in', 'line', 'the', 'hermetic']), ('the', ['line', 'with', 'hermetic', 'axiom']), ('hermetic', ['with', 'the', 'axiom', 'as']), ('axiom', ['the', 'hermetic', 'as', 'above']), ('as', ['hermetic', 'axiom', 'above', 'so']), ('above', ['axiom', 'as', 'so', 'below']), ('so', ['as', 'above', 'below', '.\\n']), ('below', ['above', 'so', '.\\n']), ('.\\n', ['so', 'below']), ('on', ['some', 'systems']), ('some', ['on', 'systems', 'control-s']), ('systems', ['on', 'some', 'control-s', 'retains']), ('control-s', ['some', 'systems', 'retains', 'its']), ('retains', ['systems', 'control-s', 'its', 'meaning']), ('its', ['control-s', 'retains', 'meaning', 'but']), ('meaning', ['retains', 'its', 'but', 'control-q']), ('but', ['its', 'meaning', 'control-q', 'is']), ('control-q', ['meaning', 'but', 'is', 'replaced']), ('is', ['but', 'control-q', 'replaced', 'by']), ('replaced', ['control-q', 'is', 'by', 'a']), ('by', ['is', 'replaced', 'a', 'second']), ('a', ['replaced', 'by', 'second', 'control-s']), ('second', ['by', 'a', 'control-s', 'to']), ('control-s', ['a', 'second', 'to', 'resume']), ('to', ['second', 'control-s', 'resume', 'output']), ('resume', ['control-s', 'to', 'output', '.\\n']), ('output', ['to', 'resume', '.\\n']), ('.\\n', ['resume', 'output']), ('her', ['tears', 'formed']), ('tears', ['her', 'formed', 'the']), ('formed', ['her', 'tears', 'the', 'river']), ('the', ['tears', 'formed', 'river', 'achelous']), ('river', ['formed', 'the', 'achelous', '.\\n']), ('achelous', ['the', 'river', '.\\n']), ('.\\n', ['river', 'achelous']), ('there', ['are', 'three']), ('are', ['there', 'three', 'systems']), ('three', ['there', 'are', 'systems', 'of']), ('systems', ['are', 'three', 'of', 'schools']), ('of', ['three', 'systems', 'schools', '–']), ('schools', ['systems', 'of', '–', 'andorran']), ('–', ['of', 'schools', 'andorran', ',']), ('andorran', ['schools', '–', ',', 'french']), (',', ['–', 'andorran', 'french', 'and']), ('french', ['andorran', ',', 'and', 'spanish']), ('and', [',', 'french', 'spanish', '–']), ('spanish', ['french', 'and', '–', 'which']), ('–', ['and', 'spanish', 'which', 'use']), ('which', ['spanish', '–', 'use', 'catalan']), ('use', ['–', 'which', 'catalan', ',']), ('catalan', ['which', 'use', ',', 'french']), (',', ['use', 'catalan', 'french', 'and']), ('french', ['catalan', ',', 'and', 'spanish']), ('and', [',', 'french', 'spanish', ',']), ('spanish', ['french', 'and', ',', 'respectively']), (',', ['and', 'spanish', 'respectively', ',']), ('respectively', ['spanish', ',', ',', 'as']), (',', [',', 'respectively', 'as', 'the']), ('as', ['respectively', ',', 'the', 'main']), ('the', [',', 'as', 'main', 'language']), ('main', ['as', 'the', 'language', 'of']), ('language', ['the', 'main', 'of', 'instruction']), ('of', ['main', 'language', 'instruction', '.\\n']), ('instruction', ['language', 'of', '.\\n']), ('.\\n', ['of', 'instruction']), ('soule', ['argued', 'that']), ('argued', ['soule', 'that', 'the']), ('that', ['soule', 'argued', 'the', 'animals']), ('the', ['argued', 'that', 'animals', 'were']), ('animals', ['that', 'the', 'were', 'not']), ('were', ['the', 'animals', 'not', 'consistent']), ('not', ['animals', 'were', 'consistent', 'enough']), ('consistent', ['were', 'not', 'enough', 'with']), ('enough', ['not', 'consistent', 'with', 'their']), ('with', ['consistent', 'enough', 'their', 'real']), ('their', ['enough', 'with', 'real', 'world']), ('real', ['with', 'their', 'world', 'inspirations']), ('world', ['their', 'real', 'inspirations', ',']), ('inspirations', ['real', 'world', ',', 'and']), (',', ['world', 'inspirations', 'and', 'said']), ('and', ['inspirations', ',', 'said', ',']), ('said', [',', 'and', ',', '\"']), (',', ['and', 'said', '\"', 'it']), ('\"', ['said', ',', 'it', 'seems']), ('it', [',', '\"', 'seems', 'to']), ('seems', ['\"', 'it', 'to', 'me']), ('to', ['it', 'seems', 'me', 'that']), ('me', ['seems', 'to', 'that', 'the']), ('that', ['to', 'me', 'the', 'failure']), ('the', ['me', 'that', 'failure', 'of']), ('failure', ['that', 'the', 'of', 'this']), ('of', ['the', 'failure', 'this', 'book']), ('this', ['failure', 'of', 'book', '(']), ('book', ['of', 'this', '(', 'commercially']), ('(', ['this', 'book', 'commercially', 'it']), ('commercially', ['book', '(', 'it', 'is']), ('it', ['(', 'commercially', 'is', 'already']), ('is', ['commercially', 'it', 'already', 'assured']), ('already', ['it', 'is', 'assured', 'of']), ('assured', ['is', 'already', 'of', 'tremendous']), ('of', ['already', 'assured', 'tremendous', 'success']), ('tremendous', ['assured', 'of', 'success', ')']), ('success', ['of', 'tremendous', ')', 'arises']), (')', ['tremendous', 'success', 'arises', 'from']), ('arises', ['success', ')', 'from', 'the']), ('from', [')', 'arises', 'the', 'fact']), ('the', ['arises', 'from', 'fact', 'that']), ('fact', ['from', 'the', 'that', 'the']), ('that', ['the', 'fact', 'the', 'satire']), ('the', ['fact', 'that', 'satire', 'deals']), ('satire', ['that', 'the', 'deals', 'not']), ('deals', ['the', 'satire', 'not', 'with']), ('not', ['satire', 'deals', 'with', 'something']), ('with', ['deals', 'not', 'something', 'the']), ('something', ['not', 'with', 'the', 'author']), ('the', ['with', 'something', 'author', 'has']), ('author', ['something', 'the', 'has', 'experienced']), ('has', ['the', 'author', 'experienced', ',']), ('experienced', ['author', 'has', ',', 'but']), (',', ['has', 'experienced', 'but', 'rather']), ('but', ['experienced', ',', 'rather', 'with']), ('rather', [',', 'but', 'with', 'stereotyped']), ('with', ['but', 'rather', 'stereotyped', 'ideas']), ('stereotyped', ['rather', 'with', 'ideas', 'about']), ('ideas', ['with', 'stereotyped', 'about', 'a']), ('about', ['stereotyped', 'ideas', 'a', 'country']), ('a', ['ideas', 'about', 'country', 'which']), ('country', ['about', 'a', 'which', 'he']), ('which', ['a', 'country', 'he', 'probably']), ('he', ['country', 'which', 'probably', 'does']), ('probably', ['which', 'he', 'does', 'not']), ('does', ['he', 'probably', 'not', 'know']), ('not', ['probably', 'does', 'know', 'very']), ('know', ['does', 'not', 'very', 'well']), ('very', ['not', 'know', 'well', '\"']), ('well', ['know', 'very', '\"', '.\\n']), ('\"', ['very', 'well', '.\\n']), ('.\\n', ['well', '\"']), ('orwell', ['biographer', 'jeffrey']), ('biographer', ['orwell', 'jeffrey', 'meyers']), ('jeffrey', ['orwell', 'biographer', 'meyers', 'has']), ('meyers', ['biographer', 'jeffrey', 'has', 'written']), ('has', ['jeffrey', 'meyers', 'written', ',']), ('written', ['meyers', 'has', ',', '\"']), (',', ['has', 'written', '\"', 'virtually']), ('\"', ['written', ',', 'virtually', 'every']), ('virtually', [',', '\"', 'every', 'detail']), ('every', ['\"', 'virtually', 'detail', 'has']), ('detail', ['virtually', 'every', 'has', 'political']), ('has', ['every', 'detail', 'political', 'significance']), ('political', ['detail', 'has', 'significance', 'in']), ('significance', ['has', 'political', 'in', 'this']), ('in', ['political', 'significance', 'this', 'allegory']), ('this', ['significance', 'in', 'allegory', '.']), ('allegory', ['in', 'this', '.', '\"\\n']), ('.', ['this', 'allegory', '\"\\n']), ('\"\\n', ['allegory', '.']), ('gymnophiona', ['the', 'order']), ('the', ['gymnophiona', 'order', 'gymnophiona']), ('order', ['gymnophiona', 'the', 'gymnophiona', '(']), ('gymnophiona', ['the', 'order', '(', 'from']), ('(', ['order', 'gymnophiona', 'from', 'the']), ('from', ['gymnophiona', '(', 'the', 'greek']), ('the', ['(', 'from', 'greek', 'gymnos']), ('greek', ['from', 'the', 'gymnos', 'meaning']), ('gymnos', ['the', 'greek', 'meaning', '\"']), ('meaning', ['greek', 'gymnos', '\"', 'naked']), ('\"', ['gymnos', 'meaning', 'naked', '\"']), ('naked', ['meaning', '\"', '\"', 'and']), ('\"', ['\"', 'naked', 'and', 'ophis']), ('and', ['naked', '\"', 'ophis', 'meaning']), ('ophis', ['\"', 'and', 'meaning', '\"']), ('meaning', ['and', 'ophis', '\"', 'serpent']), ('\"', ['ophis', 'meaning', 'serpent', '\"']), ('serpent', ['meaning', '\"', '\"', ')']), ('\"', ['\"', 'serpent', ')', 'or']), (')', ['serpent', '\"', 'or', 'apoda']), ('or', ['\"', ')', 'apoda', '(']), ('apoda', [')', 'or', '(', 'from']), ('(', ['or', 'apoda', 'from', 'the']), ('from', ['apoda', '(', 'the', 'latin']), ('the', ['(', 'from', 'latin', 'an']), ('latin', ['from', 'the', 'an', '-']), ('an', ['the', 'latin', '-', 'meaning']), ('-', ['latin', 'an', 'meaning', '\"']), ('meaning', ['an', '-', '\"', 'without']), ('\"', ['-', 'meaning', 'without', '\"']), ('without', ['meaning', '\"', '\"', 'and']), ('\"', ['\"', 'without', 'and', 'the']), ('and', ['without', '\"', 'the', 'greek']), ('the', ['\"', 'and', 'greek', 'poda']), ('greek', ['and', 'the', 'poda', 'meaning']), ('poda', ['the', 'greek', 'meaning', '\"']), ('meaning', ['greek', 'poda', '\"', 'legs']), ('\"', ['poda', 'meaning', 'legs', '\"']), ('legs', ['meaning', '\"', '\"', ')']), ('\"', ['\"', 'legs', ')', 'comprise']), (')', ['legs', '\"', 'comprise', 'the']), ('comprise', ['\"', ')', 'the', 'caecilians']), ('the', [')', 'comprise', 'caecilians', '.\\n']), ('caecilians', ['comprise', 'the', '.\\n']), ('.\\n', ['the', 'caecilians']), ('other', ['satellite', 'males']), ('satellite', ['other', 'males', 'remain']), ('males', ['other', 'satellite', 'remain', 'quietly']), ('remain', ['satellite', 'males', 'quietly', 'nearby']), ('quietly', ['males', 'remain', 'nearby', ',']), ('nearby', ['remain', 'quietly', ',', 'waiting']), (',', ['quietly', 'nearby', 'waiting', 'for']), ('waiting', ['nearby', ',', 'for', 'their']), ('for', [',', 'waiting', 'their', 'opportunity']), ('their', ['waiting', 'for', 'opportunity', 'to']), ('opportunity', ['for', 'their', 'to', 'take']), ('to', ['their', 'opportunity', 'take', 'over']), ('take', ['opportunity', 'to', 'over', 'a']), ('over', ['to', 'take', 'a', 'territory']), ('a', ['take', 'over', 'territory', '.\\n']), ('territory', ['over', 'a', '.\\n']), ('.\\n', ['a', 'territory']), ('meanwhile', ['they', 'have']), ('they', ['meanwhile', 'have', 'been']), ('have', ['meanwhile', 'they', 'been', 'observed']), ('been', ['they', 'have', 'observed', 'to']), ('observed', ['have', 'been', 'to', 'ingest']), ('to', ['been', 'observed', 'ingest', 'fluid']), ('ingest', ['observed', 'to', 'fluid', 'exuded']), ('fluid', ['to', 'ingest', 'exuded', 'from']), ('exuded', ['ingest', 'fluid', 'from', 'the']), ('from', ['fluid', 'exuded', 'the', 'maternal']), ('the', ['exuded', 'from', 'maternal', 'cloaca']), ('maternal', ['from', 'the', 'cloaca', '.\\n']), ('cloaca', ['the', 'maternal', '.\\n']), ('.\\n', ['maternal', 'cloaca']), ('these', ['are', 'part']), ('are', ['these', 'part', 'of']), ('part', ['these', 'are', 'of', 'the']), ('of', ['are', 'part', 'the', 'broader']), ('the', ['part', 'of', 'broader', 'discipline']), ('broader', ['of', 'the', 'discipline', 'of']), ('discipline', ['the', 'broader', 'of', 'zoology']), ('of', ['broader', 'discipline', 'zoology', 'but']), ('zoology', ['discipline', 'of', 'but', 'are']), ('but', ['of', 'zoology', 'are', 'seldom']), ('are', ['zoology', 'but', 'seldom', 'studied']), ('seldom', ['but', 'are', 'studied', 'alone']), ('studied', ['are', 'seldom', 'alone', '.\\n']), ('alone', ['seldom', 'studied', '.\\n']), ('.\\n', ['studied', 'alone']), ('the', ['state', 'has']), ('state', ['the', 'has', 'an']), ('has', ['the', 'state', 'an', 'independence']), ('an', ['state', 'has', 'independence', 'movement']), ('independence', ['has', 'an', 'movement', 'favoring']), ('movement', ['an', 'independence', 'favoring', 'a']), ('favoring', ['independence', 'movement', 'a', 'vote']), ('a', ['movement', 'favoring', 'vote', 'on']), ('vote', ['favoring', 'a', 'on', 'secession']), ('on', ['a', 'vote', 'secession', 'from']), ('secession', ['vote', 'on', 'from', 'the']), ('from', ['on', 'secession', 'the', 'united']), ('the', ['secession', 'from', 'united', 'states']), ('united', ['from', 'the', 'states', ',']), ('states', ['the', 'united', ',', 'with']), (',', ['united', 'states', 'with', 'the']), ('with', ['states', ',', 'the', 'alaskan']), ('the', [',', 'with', 'alaskan', 'independence']), ('alaskan', ['with', 'the', 'independence', 'party']), ('independence', ['the', 'alaskan', 'party', '.\\n']), ('party', ['alaskan', 'independence', '.\\n']), ('.\\n', ['independence', 'party']), ('between', ['1700', 'and']), ('1700', ['between', 'and', '1980']), ('and', ['between', '1700', '1980', ',']), ('1980', ['1700', 'and', ',', '\"']), (',', ['and', '1980', '\"', 'the']), ('\"', ['1980', ',', 'the', 'total']), ('the', [',', '\"', 'total', 'area']), ('total', ['\"', 'the', 'area', 'of']), ('area', ['the', 'total', 'of', 'cultivated']), ('of', ['total', 'area', 'cultivated', 'land']), ('cultivated', ['area', 'of', 'land', 'worldwide']), ('land', ['of', 'cultivated', 'worldwide', 'increased']), ('worldwide', ['cultivated', 'land', 'increased', '466']), ('increased', ['land', 'worldwide', '466', '%']), ('466', ['worldwide', 'increased', '%', '\"']), ('%', ['increased', '466', '\"', 'and']), ('\"', ['466', '%', 'and', 'yields']), ('and', ['%', '\"', 'yields', 'increased']), ('yields', ['\"', 'and', 'increased', 'dramatically']), ('increased', ['and', 'yields', 'dramatically', ',']), ('dramatically', ['yields', 'increased', ',', 'particularly']), (',', ['increased', 'dramatically', 'particularly', 'because']), ('particularly', ['dramatically', ',', 'because', 'of']), ('because', [',', 'particularly', 'of', 'selectively'])]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_reader(data_path + 'wiki-corpus.50000.txt')[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Good: different subjects\n",
    "\n",
    "Bad: one type of lanuage, informative, may loose variety that can be in different kinds of texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.vocab = {}\n",
    "        for target_word, context in data:\n",
    "            if target_word not in self.vocab:\n",
    "                self.vocab[target_word] = len(self.vocab)\n",
    "            for word in context:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = len(self.vocab)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context = self.data[idx]\n",
    "\n",
    "        encoded_context = [self.vocab[word] for word in context]\n",
    "        encoded_context.extend([len(encoded_context)] * (WINDOW_SIZE - len(encoded_context)))\n",
    "\n",
    "        Sample = namedtuple('Sample', 'target_word context')\n",
    "\n",
    "        return Sample(self.vocab[target_word], encoded_context)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_data(path):\n",
    "    samples = corpus_reader(path)[:1000]\n",
    "    dataset = CustDataset(samples)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: x)\n",
    "\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x7fd2b27f3e80>"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(data_path + 'wiki-corpus.50000.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some proper nouns may be missed, those that may have a counterpart in a common noun and vice versa?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, dimensions)\n",
    "        self.prediction = nn.Linear(dimensions, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_tensor = torch.tensor(context)\n",
    "        print(context_tensor.size())\n",
    "        embedded_context = self.embeddings(context_tensor)\n",
    "        print(embedded_context.size())\n",
    "        projection = self.projection_function(embedded_context)\n",
    "        print(projection.size())\n",
    "        predictions = self.prediction(embedded_context)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs, dim=1)\n",
    "        return xs_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs': 3,\n",
    "                                   'batch_size': 16,\n",
    "                                   'embedding_size': 128,\n",
    "                                   'learning_rate': 0.001,\n",
    "                                   'embedding_dim': 128}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "dataloader = get_data(data_path + 'wiki-corpus.50000.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "cbow_model = CBOWModel(len(dataloader.dataset), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[370, 23, 371, 372], [23, 184, 518, 512], [158, 159, 30, 6], [23, 24, 23, 6], [7, 265, 267, 34], [23, 313, 314, 23], [179, 469, 16, 334], [114, 504, 7, 505]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[12, 180, 23, 232], [7, 43, 7, 44], [412, 406, 413, 414], [240, 7, 39, 484], [229, 58, 352, 353], [336, 339, 2, 2], [24, 479, 6, 480], [53, 24, 55, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[482, 440, 7, 483], [164, 24, 166, 167], [188, 23, 189, 34], [126, 143, 23, 144], [368, 369, 364, 370], [287, 288, 290, 291], [38, 323, 324, 34], [338, 336, 34, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[6, 161, 6, 10], [102, 103, 105, 16], [92, 16, 94, 95], [226, 7, 228, 23], [433, 184, 434, 435], [114, 504, 2, 2], [405, 406, 90, 408], [7, 124, 10, 126]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 24, 64, 65], [441, 231, 442, 443], [362, 23, 363, 10], [70, 23, 71, 72], [122, 123, 124, 125], [38, 39, 41, 42], [406, 19, 414, 34], [136, 10, 23, 138]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[419, 420, 23, 422], [409, 114, 509, 510], [449, 13, 450, 7], [330, 262, 34, 3], [79, 5, 80, 81], [465, 23, 466, 467], [94, 95, 10, 7], [100, 101, 103, 104]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[71, 72, 36, 7], [6, 445, 7, 489], [24, 320, 34, 3], [23, 130, 10, 132], [294, 295, 30, 297], [88, 87, 89, 90], [495, 19, 289, 30], [10, 7, 22, 23]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[19, 438, 7, 219], [138, 10, 137, 23], [24, 7, 486, 408], [177, 279, 23, 281], [336, 337, 336, 339], [302, 19, 23, 303], [212, 6, 6, 191], [22, 23, 16, 25]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 201, 34, 3], [357, 358, 359, 24], [6, 183, 185, 24], [7, 97, 99, 100], [23, 312, 313, 23], [10, 507, 114, 508], [5, 6, 8, 9], [166, 167, 34, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[409, 410, 411, 38], [201, 23, 202, 203], [236, 237, 239, 240], [39, 512, 514, 30], [438, 5, 219, 10], [7, 448, 13, 184], [49, 50, 52, 7], [194, 10, 219, 220]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[399, 16, 184, 7], [298, 299, 300, 240], [484, 408, 485, 6], [304, 305, 30, 307], [242, 171, 2, 2], [30, 154, 23, 156], [16, 271, 30, 272], [340, 10, 342, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[76, 86, 87, 34], [23, 494, 163, 495], [187, 405, 407, 90], [492, 493, 494, 262], [117, 19, 34, 3], [337, 338, 339, 34], [421, 23, 24, 423], [0, 73, 34, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[157, 6, 528, 526], [384, 385, 386, 61], [10, 132, 23, 134], [493, 23, 262, 163], [23, 49, 51, 52], [23, 6, 467, 468], [363, 10, 365, 24], [23, 187, 23, 187]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[355, 357, 358, 3], [114, 290, 2, 2], [199, 7, 201, 23], [23, 281, 282, 19], [304, 306, 30, 3], [58, 336, 338, 336], [104, 105, 30, 106], [0, 1, 3, 4]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[408, 6, 6, 445], [373, 374, 375, 23], [24, 194, 218, 219], [19, 121, 122, 123], [16, 255, 256, 29], [23, 310, 23, 312], [7, 401, 61, 403], [230, 231, 180, 100]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[13, 184, 7, 451], [325, 225, 326, 3], [436, 23, 158, 23], [143, 136, 144, 145], [416, 7, 417, 34], [7, 348, 349, 350], [61, 403, 404, 34], [206, 16, 207, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[325, 58, 326, 327], [10, 389, 2, 2], [296, 30, 16, 298], [161, 10, 23, 187], [112, 113, 16, 39], [418, 290, 405, 3], [7, 269, 62, 3], [7, 209, 210, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[263, 264, 7, 265], [278, 445, 240, 7], [242, 171, 19, 244], [23, 220, 274, 275], [179, 452, 409, 453], [364, 368, 23, 364], [341, 10, 2, 2], [72, 23, 7, 62]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[411, 38, 412, 406], [10, 419, 421, 23], [23, 7, 10, 364], [36, 173, 174, 23], [138, 10, 137, 23], [388, 10, 34, 3], [10, 524, 525, 526], [39, 484, 6, 485]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[36, 251, 252, 7], [185, 114, 2, 2], [475, 474, 240, 7], [23, 191, 203, 16], [129, 127, 130, 131], [430, 5, 7, 3], [360, 10, 362, 23], [182, 58, 183, 184]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[508, 509, 34, 3], [316, 23, 23, 318], [7, 522, 10, 524], [226, 26, 261, 262], [7, 359, 360, 10], [340, 341, 342, 343], [41, 42, 43, 10], [87, 62, 90, 91]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 134, 136, 10], [6, 10, 164, 24], [12, 13, 15, 16], [57, 58, 60, 61], [383, 384, 2, 2], [86, 16, 34, 3], [5, 30, 81, 19], [446, 240, 447, 5]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[526, 529, 530, 531], [256, 29, 258, 16], [300, 240, 23, 302], [191, 23, 12, 254], [16, 7, 23, 226], [23, 530, 10, 532], [311, 23, 23, 313], [514, 30, 52, 516]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[38, 30, 406, 19], [511, 179, 2, 2], [7, 475, 440, 240], [400, 184, 401, 402], [133, 23, 135, 136], [401, 402, 403, 381], [329, 330, 331, 34], [225, 226, 227, 228]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[125, 10, 127, 23], [119, 19, 121, 3], [23, 364, 369, 23], [7, 88, 62, 89], [488, 7, 34, 3], [371, 372, 374, 10], [19, 82, 83, 84], [127, 23, 125, 10]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[408, 409, 58, 411], [440, 240, 476, 477], [105, 16, 106, 16], [240, 301, 302, 19], [0, 2, 3, 3], [78, 79, 30, 80], [286, 5, 287, 288], [16, 334, 472, 473]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[502, 503, 2, 2], [194, 10, 196, 16], [504, 10, 505, 506], [10, 151, 153, 30], [23, 409, 184, 454], [16, 33, 2, 2], [90, 365, 24, 380], [16, 276, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[268, 269, 270, 16], [474, 7, 474, 440], [36, 7, 0, 73], [23, 229, 230, 231], [183, 184, 24, 161], [24, 381, 34, 3], [191, 171, 243, 3], [149, 150, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 76, 16, 87], [6, 231, 19, 438], [208, 209, 2, 2], [171, 243, 244, 38], [398, 399, 400, 184], [23, 128, 10, 129], [308, 7, 23, 310], [111, 112, 114, 16]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[393, 394, 2, 2], [289, 290, 292, 34], [19, 252, 253, 23], [463, 464, 465, 23], [107, 108, 110, 34], [176, 177, 2, 2], [390, 391, 393, 394], [58, 442, 10, 444]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[10, 137, 138, 10], [520, 24, 2, 2], [106, 16, 108, 109], [161, 162, 10, 163], [323, 24, 34, 3], [7, 186, 13, 432], [16, 298, 23, 300], [326, 327, 329, 330]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[95, 96, 7, 97], [155, 23, 157, 158], [23, 24, 10, 218], [309, 23, 311, 23], [271, 220, 272, 273], [23, 314, 315, 23], [96, 10, 97, 98], [244, 38, 24, 246]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[10, 334, 58, 336], [134, 135, 10, 137], [268, 269, 2, 2], [519, 24, 521, 3], [76, 77, 79, 5], [333, 10, 335, 58], [19, 118, 2, 2], [415, 416, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[266, 267, 2, 2], [17, 18, 7, 20], [26, 27, 29, 30], [114, 290, 10, 419], [285, 16, 5, 179], [192, 193, 10, 195], [211, 23, 6, 213], [10, 7, 23, 45]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[10, 361, 179, 379], [191, 242, 243, 19], [23, 144, 10, 146], [23, 230, 12, 180], [258, 16, 259, 23], [250, 36, 2, 2], [372, 373, 10, 375], [232, 233, 235, 16]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[16, 286, 179, 287], [483, 39, 408, 6], [80, 81, 82, 7], [220, 30, 273, 262], [173, 171, 23, 67], [191, 23, 214, 52], [198, 199, 200, 201], [186, 269, 432, 433]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 303, 2, 2], [356, 357, 2, 2], [7, 377, 10, 361], [223, 224, 25, 34], [171, 174, 67, 175], [180, 181, 7, 182], [395, 396, 61, 30], [30, 284, 16, 286]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 182, 6, 183], [148, 135, 150, 34], [57, 58, 2, 2], [159, 160, 6, 161], [217, 23, 194, 10], [109, 110, 2, 2], [476, 486, 6, 487], [5, 7, 449, 13]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[39, 46, 2, 2], [11, 12, 14, 15], [23, 61, 427, 428], [423, 420, 424, 425], [6, 7, 523, 10], [62, 0, 74, 34], [23, 24, 382, 34], [37, 38, 40, 41]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[179, 379, 365, 363], [16, 7, 266, 267], [36, 37, 39, 40], [5, 7, 10, 334], [23, 45, 46, 34], [10, 364, 24, 366], [524, 112, 526, 527], [402, 61, 381, 404]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[6, 7, 9, 10], [474, 440, 7, 476], [187, 189, 2, 2], [52, 223, 23, 25], [272, 273, 7, 268], [443, 10, 278, 445], [431, 5, 186, 269], [352, 460, 34, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[59, 60, 7, 62], [426, 23, 7, 427], [21, 22, 24, 16], [112, 113, 2, 2], [293, 294, 296, 30], [334, 471, 473, 3], [36, 214, 215, 216], [7, 221, 222, 52]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[454, 455, 30, 456], [23, 24, 135, 149], [171, 395, 390, 61], [273, 262, 268, 23], [246, 171, 248, 34], [8, 9, 7, 11], [349, 350, 351, 23], [140, 137, 141, 142]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[512, 513, 30, 515], [269, 13, 433, 184], [422, 24, 420, 229], [30, 515, 516, 240], [501, 240, 502, 503], [509, 510, 2, 2], [7, 148, 23, 184], [58, 386, 387, 7]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 44, 45, 39], [69, 70, 7, 71], [24, 360, 361, 362], [439, 440, 231, 58], [190, 23, 191, 36], [67, 170, 171, 172], [16, 17, 19, 7], [7, 98, 344, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[30, 397, 398, 399], [6, 24, 408, 6], [387, 7, 10, 389], [10, 152, 153, 3], [24, 521, 6, 7], [363, 24, 23, 24], [457, 458, 459, 352], [179, 180, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[151, 152, 30, 154], [490, 491, 2, 2], [278, 10, 279, 280], [77, 78, 5, 30], [476, 477, 6, 478], [10, 205, 16, 200], [218, 219, 7, 221], [81, 19, 7, 83]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[119, 120, 121, 23], [141, 142, 143, 136], [10, 222, 223, 224], [18, 19, 20, 10], [527, 157, 24, 528], [58, 59, 61, 7], [295, 296, 297, 16], [249, 250, 251, 19]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[209, 210, 23, 212], [67, 170, 2, 2], [151, 152, 2, 2], [16, 93, 95, 96], [294, 295, 2, 2], [137, 23, 10, 140], [435, 436, 24, 158], [364, 370, 7, 371]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[366, 367, 364, 368], [470, 16, 471, 472], [403, 381, 34, 3], [61, 387, 388, 10], [35, 36, 38, 39], [397, 10, 399, 16], [7, 114, 322, 3], [270, 16, 220, 30]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[284, 285, 286, 5], [395, 396, 2, 2], [75, 77, 78, 3], [280, 23, 36, 282], [23, 7, 72, 23], [10, 177, 280, 23], [61, 7, 63, 0], [378, 10, 362, 179]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[479, 408, 480, 6], [15, 16, 18, 19], [24, 380, 24, 381], [30, 35, 37, 38], [204, 10, 206, 16], [444, 278, 446, 240], [239, 240, 34, 3], [23, 24, 25, 7]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[90, 91, 16, 93], [406, 407, 408, 409], [85, 7, 86, 16], [51, 52, 53, 24], [65, 66, 68, 69], [55, 490, 343, 492], [318, 23, 24, 320], [445, 488, 489, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 156, 158, 159], [56, 58, 59, 3], [24, 7, 65, 66], [240, 7, 5, 7], [159, 233, 2, 2], [56, 57, 59, 60], [91, 92, 93, 94], [113, 114, 39, 115]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[455, 108, 456, 229], [73, 74, 2, 2], [170, 5, 172, 36], [344, 24, 337, 346], [23, 364, 23, 7], [58, 6, 184, 185], [262, 331, 2, 2], [293, 295, 296, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[249, 36, 251, 3], [334, 335, 336, 337], [480, 6, 481, 482], [103, 104, 16, 30], [10, 163, 24, 165], [7, 333, 10, 3], [179, 465, 6, 466], [7, 64, 66, 67]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[343, 7, 10, 344], [7, 11, 13, 14], [342, 343, 98, 10], [7, 219, 334, 439], [89, 90, 92, 16], [343, 492, 23, 494], [121, 23, 192, 193], [23, 36, 62, 0]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[220, 7, 10, 222], [30, 456, 67, 457], [30, 412, 19, 413], [434, 435, 23, 24], [23, 67, 24, 176], [461, 462, 464, 179], [199, 7, 2, 2], [351, 23, 58, 30]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[235, 16, 237, 238], [132, 133, 134, 135], [255, 19, 29, 257], [528, 526, 23, 530], [100, 23, 233, 234], [1, 2, 4, 5], [477, 408, 478, 6], [486, 408, 487, 6]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[136, 23, 148, 135], [14, 15, 17, 18], [58, 30, 353, 354], [485, 6, 7, 476], [7, 274, 16, 276], [158, 23, 231, 437], [208, 209, 211, 23], [247, 248, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[422, 24, 23, 426], [19, 7, 10, 7], [2, 3, 5, 6], [38, 245, 246, 171], [7, 371, 373, 374], [240, 7, 503, 34], [16, 25, 26, 27], [152, 153, 154, 155]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[240, 241, 2, 2], [275, 16, 34, 3], [184, 450, 451, 179], [6, 213, 191, 23], [7, 363, 364, 365], [381, 382, 2, 2], [513, 514, 515, 52], [13, 459, 460, 6]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[374, 10, 23, 376], [6, 485, 24, 7], [30, 307, 7, 309], [490, 491, 492, 493], [365, 363, 380, 23], [7, 8, 10, 7], [341, 10, 343, 7], [500, 501, 7, 502]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 384, 385, 3], [7, 476, 408, 6], [19, 413, 34, 3], [88, 87, 2, 2], [191, 202, 16, 7], [29, 257, 16, 7], [10, 7, 506, 10], [7, 21, 23, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[62, 270, 271, 220], [432, 433, 163, 434], [24, 324, 2, 2], [279, 280, 281, 36], [233, 234, 16, 236], [7, 83, 85, 7], [172, 36, 171, 174], [64, 65, 67, 68]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[468, 179, 470, 16], [42, 7, 10, 7], [332, 333, 334, 335], [518, 512, 34, 3], [24, 16, 7, 26], [50, 51, 7, 53], [7, 383, 385, 58], [58, 225, 327, 328]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[525, 526, 157, 6], [52, 215, 23, 217], [427, 428, 429, 34], [24, 176, 34, 3], [231, 58, 443, 10], [75, 76, 78, 79], [391, 392, 2, 2], [23, 6, 522, 523]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[16, 200, 34, 3], [277, 7, 10, 177], [83, 84, 7, 76], [30, 238, 2, 2], [5, 171, 36, 173], [7, 208, 210, 211], [24, 158, 6, 231], [39, 115, 10, 117]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[19, 283, 284, 285], [27, 28, 30, 31], [24, 349, 16, 351], [319, 24, 321, 34], [497, 159, 180, 498], [121, 23, 123, 7], [16, 7, 10, 205], [178, 179, 181, 5]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[392, 393, 34, 3], [147, 136, 24, 148], [159, 233, 498, 19], [328, 329, 262, 331], [362, 179, 90, 365], [305, 306, 2, 2], [201, 417, 2, 2], [445, 446, 7, 447]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 47, 2, 2], [10, 126, 23, 128], [461, 463, 464, 3], [7, 227, 23, 229], [187, 188, 187, 189], [24, 423, 229, 424], [237, 238, 240, 241], [16, 47, 48, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 489, 2, 2], [408, 6, 6, 24], [522, 523, 524, 112], [391, 392, 394, 34], [6, 24, 526, 529], [274, 275, 276, 34], [10, 344, 345, 337], [191, 225, 7, 227]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[238, 239, 241, 34], [98, 10, 24, 345], [7, 447, 7, 448], [195, 196, 197, 34], [16, 87, 2, 2], [471, 472, 2, 2], [507, 409, 508, 509], [458, 13, 352, 460]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[478, 6, 479, 408], [24, 54, 34, 3], [425, 23, 24, 423], [413, 414, 2, 2], [184, 454, 108, 30], [12, 254, 255, 19], [7, 502, 34, 3], [159, 347, 348, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[310, 311, 312, 23], [16, 197, 2, 2], [7, 253, 191, 23], [19, 496, 30, 238], [99, 100, 102, 103], [32, 16, 34, 3], [497, 233, 180, 3], [7, 53, 54, 55]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 451, 452, 23], [7, 518, 41, 34], [23, 159, 7, 348], [347, 7, 24, 349], [437, 19, 5, 7], [19, 223, 303, 34], [76, 77, 2, 2], [157, 158, 160, 30]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[431, 5, 2, 2], [375, 23, 7, 377], [215, 216, 217, 23], [171, 247, 34, 3], [420, 229, 425, 23], [9, 10, 11, 12], [335, 58, 337, 338], [243, 19, 38, 245]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 94, 36, 121], [124, 125, 126, 127], [314, 23, 23, 316], [224, 23, 34, 3], [519, 520, 521, 23], [153, 30, 155, 23], [63, 0, 24, 7], [23, 319, 320, 321]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[163, 164, 165, 166], [10, 444, 445, 446], [277, 278, 10, 3], [135, 149, 34, 3], [125, 10, 127, 23], [352, 353, 34, 3], [202, 203, 7, 204], [175, 24, 177, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[410, 58, 38, 30], [68, 69, 23, 7], [185, 114, 38, 323], [245, 24, 171, 247], [6, 191, 36, 214], [407, 90, 409, 410], [376, 7, 378, 10], [61, 7, 428, 10]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[36, 121, 171, 192], [190, 23, 2, 2], [23, 191, 152, 12], [10, 7, 98, 99], [7, 47, 23, 49], [481, 482, 240, 7], [283, 30, 285, 16], [120, 19, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[47, 48, 49, 50], [24, 345, 346, 23], [6, 480, 445, 481], [67, 68, 70, 23], [516, 240, 148, 517], [10, 186, 187, 188], [136, 23, 145, 10], [464, 179, 23, 6]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[337, 346, 159, 347], [116, 10, 19, 118], [222, 52, 224, 23], [228, 23, 23, 230], [252, 7, 23, 191], [16, 236, 238, 239], [198, 7, 200, 3], [126, 127, 128, 125]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[30, 297, 298, 299], [16, 400, 7, 401], [23, 138, 140, 137], [327, 328, 330, 262], [171, 172, 173, 171], [112, 525, 527, 157], [28, 29, 31, 32], [390, 61, 397, 10]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 171, 193, 194], [23, 152, 254, 16], [377, 378, 361, 362], [30, 36, 37, 3], [24, 148, 149, 150], [97, 98, 100, 101], [191, 226, 7, 3], [174, 23, 175, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[128, 125, 129, 127], [442, 443, 444, 278], [123, 7, 125, 10], [254, 16, 19, 256], [229, 67, 458, 13], [7, 475, 2, 2], [453, 184, 455, 108], [529, 23, 531, 10]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[408, 6, 6, 445], [24, 246, 247, 248], [180, 498, 499, 500], [67, 175, 176, 177], [265, 266, 34, 3], [114, 16, 115, 116], [460, 6, 2, 2], [193, 194, 195, 196]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[334, 439, 441, 231], [462, 463, 2, 2], [358, 7, 24, 360], [7, 427, 10, 429], [16, 107, 109, 110], [290, 405, 419, 420], [469, 470, 334, 471], [320, 321, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[10, 361, 23, 7], [221, 10, 52, 223], [348, 24, 350, 16], [10, 398, 16, 400], [526, 527, 6, 24], [487, 6, 488, 7], [19, 499, 501, 240], [262, 163, 19, 496]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[44, 23, 39, 46], [390, 392, 393, 3], [428, 10, 34, 3], [261, 262, 263, 264], [23, 422, 423, 420], [169, 170, 5, 3], [16, 7, 48, 23], [154, 155, 156, 157]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 138, 139, 137], [45, 39, 34, 3], [10, 7, 12, 13], [179, 180, 5, 7], [24, 161, 186, 23], [7, 483, 484, 408], [386, 61, 7, 388], [115, 116, 117, 19]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 62, 0, 23], [10, 429, 2, 2], [290, 291, 34, 3], [240, 7, 477, 408], [299, 23, 240, 301], [520, 24, 23, 6], [137, 23, 142, 126], [396, 390, 30, 397]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[186, 23, 188, 23], [6, 466, 468, 179], [365, 24, 367, 23], [332, 333, 2, 2], [137, 23, 10, 139], [120, 19, 23, 122], [3, 4, 6, 7], [122, 504, 10, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[148, 517, 184, 7], [356, 357, 7, 359], [7, 20, 7, 21], [7, 278, 2, 2], [23, 141, 126, 143], [7, 200, 23, 191], [23, 212, 213, 6], [30, 352, 354, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[171, 396, 390, 3], [184, 185, 161, 10], [101, 102, 104, 105], [506, 10, 409, 114], [98, 99, 101, 102], [1, 2, 2, 2], [512, 41, 2, 2], [5, 179, 288, 289]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[60, 61, 62, 63], [13, 14, 16, 17], [229, 424, 23, 422], [262, 7, 23, 220], [491, 343, 493, 23], [7, 62, 73, 74], [210, 211, 212, 6], [196, 16, 34, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[452, 23, 453, 184], [6, 24, 476, 486], [225, 326, 328, 329], [135, 136, 137, 23], [5, 7, 269, 13], [517, 23, 7, 518], [6, 445, 482, 440], [20, 10, 21, 22]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[16, 39, 116, 10], [359, 24, 10, 361], [23, 7, 372, 373], [90, 408, 410, 58], [39, 40, 42, 7], [24, 165, 167, 168], [312, 23, 23, 314], [160, 30, 161, 162]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[345, 337, 23, 159], [145, 10, 147, 136], [7, 332, 10, 334], [451, 179, 23, 409], [7, 505, 10, 507], [23, 300, 301, 23], [108, 30, 229, 67], [415, 416, 201, 417]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[7, 259, 226, 26], [23, 229, 30, 352], [7, 26, 28, 29], [231, 12, 100, 23], [369, 23, 370, 23], [62, 63, 23, 24], [144, 145, 146, 147], [430, 431, 7, 186]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[301, 23, 19, 223], [7, 263, 16, 7], [288, 289, 291, 292], [7, 71, 23, 36], [178, 180, 181, 3], [223, 23, 34, 3], [23, 217, 24, 194], [379, 90, 363, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[494, 262, 495, 19], [450, 7, 179, 452], [7, 388, 389, 34], [364, 365, 366, 367], [67, 457, 13, 459], [163, 434, 436, 23], [184, 163, 435, 436], [440, 441, 58, 442]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[355, 356, 358, 7], [30, 31, 16, 33], [200, 201, 191, 202], [82, 7, 84, 85], [459, 352, 6, 34], [203, 16, 204, 10], [24, 528, 529, 23], [52, 187, 406, 407]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[264, 16, 265, 266], [350, 16, 23, 229], [505, 506, 507, 409], [139, 137, 138, 10], [177, 416, 7, 3], [167, 168, 2, 2], [52, 516, 7, 148], [7, 476, 408, 6]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[353, 354, 2, 2], [24, 366, 23, 364], [169, 67, 5, 171], [30, 106, 107, 108], [16, 23, 94, 3], [282, 19, 30, 284], [108, 109, 34, 3], [16, 351, 229, 58]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[409, 453, 454, 455], [322, 38, 24, 324], [499, 500, 240, 7], [440, 240, 483, 39], [523, 10, 112, 525], [205, 206, 200, 207], [317, 23, 23, 319], [23, 122, 7, 124]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[180, 100, 232, 233], [418, 114, 405, 10], [10, 146, 136, 23], [447, 5, 448, 449], [23, 426, 61, 7], [10, 139, 23, 138], [7, 511, 39, 512], [7, 185, 322, 38]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[23, 25, 2, 2], [165, 166, 168, 34], [179, 39, 513, 514], [216, 23, 23, 24], [130, 131, 132, 133], [122, 114, 10, 7], [251, 19, 7, 253], [23, 232, 234, 235]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[0, 23, 7, 64], [84, 85, 76, 86], [496, 289, 238, 34], [305, 306, 307, 308], [315, 23, 23, 317], [313, 23, 23, 315], [23, 6, 437, 19], [260, 261, 7, 263]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[163, 495, 496, 289], [7, 309, 310, 311], [405, 10, 420, 421], [114, 322, 323, 24], [10, 129, 23, 130], [5, 7, 58, 6], [66, 67, 69, 70], [23, 302, 223, 23]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[10, 375, 376, 7], [6, 487, 445, 488], [227, 228, 229, 23], [367, 23, 368, 369], [7, 268, 62, 270], [191, 36, 23, 171], [111, 113, 114, 3], [383, 384, 58, 386]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[467, 468, 469, 470], [7, 278, 177, 279], [23, 315, 316, 23], [181, 5, 182, 58], [55, 491, 343, 3], [456, 229, 457, 458], [259, 23, 26, 260], [6, 478, 24, 479]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[31, 32, 33, 34], [184, 7, 512, 41], [185, 24, 10, 186], [361, 362, 7, 363], [380, 23, 381, 382], [219, 10, 439, 440], [30, 6, 162, 6], [7, 87, 62, 3]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[474, 475, 474, 3], [152, 12, 16, 255], [381, 404, 2, 2], [10, 218, 220, 7], [10, 140, 23, 141], [48, 23, 50, 51], [4, 5, 7, 8], [19, 244, 245, 24]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[240, 7, 517, 23], [385, 58, 61, 387], [291, 292, 2, 2], [445, 481, 440, 240], [225, 226, 2, 2], [179, 287, 289, 290], [515, 52, 240, 7], [10, 342, 7, 98]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[171, 192, 194, 10], [58, 225, 2, 2], [23, 316, 317, 23], [234, 235, 236, 237], [30, 80, 19, 82], [54, 55, 2, 2], [43, 10, 44, 23], [23, 317, 318, 23]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[200, 207, 2, 2], [36, 282, 283, 30], [346, 23, 347, 7], [448, 449, 184, 450], [220, 7, 275, 16], [361, 362, 379, 90], [16, 30, 16, 107], [231, 437, 438, 5]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[24, 423, 426, 23], [94, 191, 121, 23], [423, 23, 23, 61], [257, 258, 7, 259], [61, 30, 10, 398], [250, 36, 19, 252], [23, 36, 52, 215], [10, 117, 118, 34]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[281, 36, 19, 283], [408, 6, 6, 24], [29, 30, 32, 16], [7, 268, 220, 7], [93, 94, 96, 10], [52, 405, 406, 3], [511, 179, 512, 513], [13, 432, 184, 163]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[131, 10, 133, 23], [127, 23, 131, 10], [187, 405, 2, 2], [23, 187, 34, 3], [52, 7, 24, 54], [177, 415, 7, 201], [219, 220, 221, 10], [156, 157, 159, 160]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[16, 190, 94, 191], [269, 62, 16, 271], [58, 411, 30, 412], [23, 422, 423, 23], [214, 52, 216, 23], [10, 195, 16, 197], [7, 179, 39, 3], [297, 16, 299, 23]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[289, 30, 34, 3], [233, 180, 19, 499], [268, 23, 7, 274], [40, 41, 7, 43], [10, 334, 440, 441], [23, 376, 377, 378], [307, 308, 309, 23], [7, 204, 205, 206]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[424, 425, 422, 24], [498, 19, 500, 501], [30, 272, 262, 7], [142, 126, 136, 23], [114, 508, 510, 34], [229, 23, 231, 12], [23, 318, 319, 24], [306, 30, 308, 7]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[521, 23, 7, 522], [466, 467, 179, 469], [26, 260, 262, 7], [262, 7, 264, 16], [184, 7, 402, 61], [23, 226, 260, 261], [146, 147, 23, 24], [35, 36, 2, 2]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n",
      "[[253, 23, 23, 152], [420, 421, 422, 24], [162, 6, 163, 164], [462, 463, 179, 465], [25, 7, 27, 28], [62, 89, 91, 92], [213, 6, 23, 36], [19, 256, 257, 258]]\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 128])\n",
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    contexts = [sample.context for sample in batch]\n",
    "    target_words = [sample.target_word for sample in batch]\n",
    "    output = cbow_model(contexts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data(data_path + 'wiki-corpus.50000.txt')\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        context = batch.context\n",
    "        target_word = batch.target_word\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(total_loss / (i + 1), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        #...\n",
    "\n",
    "        # update parameters\n",
    "        #...\n",
    "\n",
    "        # reset gradients\n",
    "        #...\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data(data_dir + 'wiki-corpus.50000.txt')\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        context = batch.context\n",
    "        target_word = batch.target_word\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(total_loss / (i + 1), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        ...\n",
    "\n",
    "        # update parameters\n",
    "        ...\n",
    "\n",
    "        # reset gradients\n",
    "        ...\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable on Canvas under files/assignments/03-lab-data). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = f.split()\n",
    "\n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "\n",
    "            # get the index for the word\n",
    "            word1_idx = ...\n",
    "            word2_idx = ...\n",
    "\n",
    "            # get the embedding of the word\n",
    "            word1_emb = ...\n",
    "            word2_emb = ...\n",
    "\n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity(...)\n",
    "\n",
    "            model_sims.append(cosine_similarity.item())\n",
    "\n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(...)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-corpus.50000.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs': 3,\n",
    "                      'batch_size': 16,\n",
    "                      'learning_rate': 0.001,\n",
    "                      'embedding_dim': 128,\n",
    "                      'output_dim': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'wiki-corpus.txt'\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "# your code here, roughly the same as for the word2vec dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(...):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = ...\n",
    "        self.LSTM = ...\n",
    "        self.predict_word = ...\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded_seq = ...\n",
    "        timestep_reprentation, *_ = ...\n",
    "        predicted_words = ...\n",
    "\n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data(...)\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab),\n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=lm_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentence\n",
    "\n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = ...\n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "\n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        gold_data = ...\n",
    "\n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print average loss for the epoch\n",
    "        print(total_loss / (i + 1), end='\\r')\n",
    "\n",
    "        # compute gradients\n",
    "        ...\n",
    "\n",
    "        # update parameters\n",
    "        ...\n",
    "\n",
    "        # reset gradients\n",
    "        ...\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "\n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = ...\n",
    "            tok_bad_s = ...\n",
    "\n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([_ for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([_ for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "\n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = LM_withLSTM(enc_good_s)\n",
    "            bad_s = LM_withLSTM(enc_bad_s)\n",
    "\n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(...)\n",
    "            bs_probs = F.softmax(...)\n",
    "\n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "\n",
    "            accuracy.append(int(gs_sent_prob > bs_sent_prob))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = ...\n",
    "        probs.append(prob)\n",
    "    sentence_prob = ...\n",
    "    return sentence_prob\n",
    "\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, ..., ...)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}